<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>Notes on PyTorch Tensor Data Types - jdhao's blog</title>
<meta name=renderer content="webkit">
<meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1">
<meta http-equiv=cache-control content="no-transform">
<meta http-equiv=cache-control content="no-siteapp">
<meta name=theme-color content="#f8f5ec">
<meta name=msapplication-navbutton-color content="#f8f5ec">
<meta name=apple-mobile-web-app-capable content="yes">
<meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec">
<meta name=author content="jdhao"><meta name=description content="In PyTorch, Tensor is the primary object that we deal with (Variable is just a thin wrapper class for Tensor). In this post, I will give a summary of pitfalls that we should avoid when using Tensors. Since FloatTensor and LongTensor are the most popular Tensor types in PyTorch, I will focus on these two data types.
"><meta name=keywords content="Hugo,theme,even">
<meta name=google-site-verification content="HTz0VHxqny_b0FfS774dICLBzHGBZCb_S11j_akF1Tw">
<meta name=generator content="Hugo 0.92.1 with theme even">
<link rel=canonical href=https://jdhao.github.io/2017/11/15/pytorch-datatype-note/>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png>
<link rel=manifest href=/manifest.json>
<link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<meta name=referrer content="no-referrer-when-downgrade"><script async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script>
<script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"ca-pub-6058871559165202",enable_page_level_ads:!0})</script>
<link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous>
<meta property="og:title" content="Notes on PyTorch Tensor Data Types">
<meta property="og:description" content="In PyTorch,
Tensor is the
primary object that we deal with (Variable is just a thin wrapper class for
Tensor). In this post, I will give a summary of pitfalls that we should avoid
when using Tensors. Since FloatTensor and LongTensor are the most popular
Tensor types in PyTorch, I will focus on these two data types.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://jdhao.github.io/2017/11/15/pytorch-datatype-note/"><meta property="article:section" content="post">
<meta property="article:published_time" content="2017-11-15T21:22:45+08:00">
<meta property="article:modified_time" content="2021-10-16T21:43:12+08:00">
<meta itemprop=name content="Notes on PyTorch Tensor Data Types">
<meta itemprop=description content="In PyTorch,
Tensor is the
primary object that we deal with (Variable is just a thin wrapper class for
Tensor). In this post, I will give a summary of pitfalls that we should avoid
when using Tensors. Since FloatTensor and LongTensor are the most popular
Tensor types in PyTorch, I will focus on these two data types."><meta itemprop=datePublished content="2017-11-15T21:22:45+08:00">
<meta itemprop=dateModified content="2021-10-16T21:43:12+08:00">
<meta itemprop=wordCount content="745">
<meta itemprop=keywords content="PyTorch,"><meta name=twitter:card content="summary">
<meta name=twitter:title content="Notes on PyTorch Tensor Data Types">
<meta name=twitter:description content="In PyTorch,
Tensor is the
primary object that we deal with (Variable is just a thin wrapper class for
Tensor). In this post, I will give a summary of pitfalls that we should avoid
when using Tensors. Since FloatTensor and LongTensor are the most popular
Tensor types in PyTorch, I will focus on these two data types."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]-->
</head>
<body>
<div id=mobile-navbar class=mobile-navbar>
<div class=mobile-header-logo>
<a href=/ class=logo>jdhao's blog</a>
</div>
<div class=mobile-navbar-icon>
<span></span>
<span></span>
<span></span>
</div>
</div>
<nav id=mobile-menu class="mobile-menu slideout-menu">
<ul class=mobile-menu-list>
<a href=/>
<li class=mobile-menu-item>Home</li>
</a><a href=/post/>
<li class=mobile-menu-item>Archives</li>
</a><a href=/categories/>
<li class=mobile-menu-item>Categories</li>
</a><a href=/tags/>
<li class=mobile-menu-item>Tags</li>
</a><a href=/about/>
<li class=mobile-menu-item>About</li>
</a>
</ul>
</nav>
<div class=container id=mobile-panel>
<header id=header class=header>
<div class=logo-wrapper>
<a href=/ class=logo>jdhao's blog</a>
</div>
<nav class=site-navbar>
<ul id=menu class=menu>
<li class=menu-item>
<a class=menu-item-link href=/>Home</a>
</li><li class=menu-item>
<a class=menu-item-link href=/post/>Archives</a>
</li><li class=menu-item>
<a class=menu-item-link href=/categories/>Categories</a>
</li><li class=menu-item>
<a class=menu-item-link href=/tags/>Tags</a>
</li><li class=menu-item>
<a class=menu-item-link href=/about/>About</a>
</li>
</ul>
</nav>
</header>
<main id=main class=main>
<div class=content-wrapper>
<div id=content class=content>
<article class=post>
<header class=post-header>
<h1 class=post-title>Notes on PyTorch Tensor Data Types</h1>
<div class=post-meta>
<span class=post-time> 2017-11-15 </span>
<span class=more-meta> 745 words </span>
<span class=more-meta> 4 mins read </span>
<span id=busuanzi_container_page_pv class=more-meta> <span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read </span>
</div>
</header>
<div class=post-toc id=post-toc>
<h2 class=post-toc-title>Contents</h2>
<div class=post-toc-content>
<nav id=TableOfContents>
<ol>
<li><a href=#tensor-operations>Tensor operations</a>
<ol>
<li><a href=#tensor-and-tensor-operation>Tensor and Tensor operation</a></li>
<li><a href=#tensor-and-scalar-operation>Tensor and scalar operation</a></li>
</ol>
</li>
<li><a href=#why-do-some-losses-require-target-to-be-longtensor>Why do some losses require target to be LongTensor?</a></li>
<li><a href=#floattensor-or-doubletensor>FloatTensor or DoubleTensor</a></li>
<li><a href=#numpy-array-and-torch-tensor>NumPy array and torch Tensor</a>
<ol>
<li><a href=#shared-memory-or-not>Shared memory or not?</a></li>
<li><a href=#correpsondece-between-numpy-and-torch-data-type>Correpsondece between NumPy and torch data type</a></li>
<li><a href=#speed-comparison-between-numpy-and-torch-operations>Speed comparison between NumPy and torch operations</a></li>
</ol>
</li>
<li><a href=#convert-scalar-to-torch-tensor>Convert scalar to torch Tensor</a></li>
<li><a href=#references>References</a></li>
</ol>
</nav>
</div>
</div>
<div class=post-content>
<p>In PyTorch,
<a href=http://pytorch.org/docs/master/tensors.html#torch-tensor><code>Tensor</code></a> is the
primary object that we deal with (<code>Variable</code> is just a thin wrapper class for
<code>Tensor</code>). In this post, I will give a summary of pitfalls that we should avoid
when using Tensors. Since <code>FloatTensor</code> and <code>LongTensor</code> are the most popular
<code>Tensor</code> types in PyTorch, I will focus on these two data types.</p>
<h1 id=tensor-operations>Tensor operations</h1>
<h2 id=tensor-and-tensor-operation>Tensor and Tensor operation</h2>
<p>For operations between Tensors, the rule is strict. Both Tensors of the
operation must have the same data type, or you will see error messages like</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback>TypeError: sub received an invalid combination of arguments - got (float), but expected one of:
 * (int value)
      didn&#39;t match because some of the arguments have invalid types: (!float!)
 * (torch.LongTensor other)
      didn&#39;t match because some of the arguments have invalid types: (!float!)
 * (int value, torch.LongTensor other)
</code></pre></div><p>As another example, several loss functions like
<a href=http://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss><code>CrossEntropyLoss</code></a>
require that the target should be torch <code>LongTensor</code>. So before doing
operations, make sure that your input <code>Tensor</code> types match the function
definitions.</p>
<p>It is easy to convert the type of one <code>Tensor</code> to another <code>Tensor</code>. Suppose <code>x</code>
and <code>y</code> are <code>Tensor</code> of different types. You can use <code>x.type(y.type())</code> or
<code>x.type_as(y)</code> to convert <code>x</code> to the type of <code>y</code>.</p>
<h2 id=tensor-and-scalar-operation>Tensor and scalar operation</h2>
<p>For <code>FloatTensor</code>, you can do math operations (multiplication, addition,
division etc.) with a scalar of type <code>int</code> or <code>float</code>. But for <code>LongTensor</code>,
you can only do math operation with <code>int</code> scalar <strong>but not <code>float</code></strong>.</p>
<h1 id=why-do-some-losses-require-target-to-be-longtensor>Why do some losses require target to be LongTensor?</h1>
<p><a href="https://discuss.pytorch.org/t/problems-with-target-arrays-of-int-int32-types-in-loss-functions/140/3?u=jdhao">According to PyTorch
developers</a>,
some use cases requires that the target be <code>LongTensor</code> type and int just can
not hold the target value.</p>
<h1 id=floattensor-or-doubletensor>FloatTensor or DoubleTensor</h1>
<p>For deep learning, precision is not a very important issue. Plus, GPU can not
process double precision very well. So <code>FloatTensor</code> is enough, which is also
the default type for model parameters.</p>
<h1 id=numpy-array-and-torch-tensor>NumPy array and torch Tensor</h1>
<h2 id=shared-memory-or-not>Shared memory or not?</h2>
<p>You can use <code>torch.from_numpy()</code> method to convert a NumPy array to
corresponding torch <code>Tensor</code>, which will share underlying memory with NumPy
array. To convert <code>Tensor</code> <code>x</code> to NumPy array, use <code>x.numpy()</code> to convert it to
a NumPy array, which also shares the memory with original <code>Tensor</code>.</p>
<p>Does torch <code>Tensor</code> and Numpy array always share the underlying memory? The
short answer is no. If their underlying data type is not compatible, a copy of
original data will be made. For example, if you try to save torch <code>FloatTensor</code>
as numpy array of type <code>np.float64</code>, it will trigger a deep copy.</p>
<h2 id=correpsondece-between-numpy-and-torch-data-type>Correpsondece between NumPy and torch data type</h2>
<p>It should be noted that not all NumPy arrays can be converted to torch
<code>Tensor</code>. Below is a table showing NumPy data types which is convertable to
torch <code>Tensor</code> type.</p>
<table>
<thead>
<tr>
<th>NumPy data type</th>
<th>Tensor data type</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>numpy.uint8</code></td>
<td><code>torch.ByteTensor</code></td>
</tr>
<tr>
<td><code>numpy.int16</code></td>
<td><code>torch.ShortTensor</code></td>
</tr>
<tr>
<td><code>numpy.int32</code></td>
<td><code>torch.IntTensor</code></td>
</tr>
<tr>
<td><code>numpy.int</code></td>
<td><code>torch.LongTensor</code></td>
</tr>
<tr>
<td><code>numpy.int64</code></td>
<td><code>torch.LongTensor</code></td>
</tr>
<tr>
<td><code>numpy.float32</code></td>
<td><code>torch.FloatTensor</code></td>
</tr>
<tr>
<td><code>numpy.float</code></td>
<td><code>torch.DoubleTensor</code></td>
</tr>
<tr>
<td><code>numpy.float64</code></td>
<td><code>torch.DoubleTensor</code></td>
</tr>
</tbody>
</table>
<h2 id=speed-comparison-between-numpy-and-torch-operations>Speed comparison between NumPy and torch operations</h2>
<p>I am curious to know the speed difference between torch Tensor operation and
equivalent NumPy ndarray operations. I do it in Jupyter-console using the
built-in magic <code>%timeit</code>.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>

<span class=c1># torch Tensor on CPU</span>

<span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>64</span><span class=p>)</span>
<span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>5000</span><span class=p>,</span> <span class=mi>64</span><span class=p>)</span>
<span class=o>%</span><span class=n>timeit</span> <span class=n>z</span><span class=o>=</span><span class=p>(</span><span class=n>x</span><span class=o>*</span><span class=n>y</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

<span class=c1># torch Tensor on GPU</span>

<span class=n>x</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>cuda</span><span class=p>(),</span> <span class=n>y</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
<span class=o>%</span><span class=n>timeit</span> <span class=n>z</span> <span class=o>=</span> <span class=p>(</span><span class=n>x</span><span class=o>*</span><span class=n>y</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

<span class=c1># numpy ndarray on CPU</span>

<span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>64</span><span class=p>))</span>
<span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>((</span><span class=mi>5000</span><span class=p>,</span> <span class=mi>64</span><span class=p>))</span>
<span class=o>%</span><span class=n>timeit</span> <span class=n>z</span> <span class=o>=</span> <span class=p>(</span><span class=n>x</span><span class=o>*</span><span class=n>y</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</code></pre></div><p>The result is listed on the following table:</p>
<table>
<thead>
<tr>
<th>Data type and device</th>
<th>Average operation time</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tensor on CPU</td>
<td>938 $\mu s$</td>
</tr>
<tr>
<td>Tensor on GPU</td>
<td>38.9 $\mu s$</td>
</tr>
<tr>
<td>NumPy ndarray (on CPU)</td>
<td>623 $\mu s$</td>
</tr>
</tbody>
</table>
<p>It is pretty clear that Tensor operations on GPU runs orders of magnitute
faster than operations on CPU. NumPy, due to its excellent implementation of
its core in C, runs a little bit faster than Tensor on CPU.</p>
<h1 id=convert-scalar-to-torch-tensor>Convert scalar to torch Tensor</h1>
<p>You can convert a scalar to <code>Tensor</code> by providing the scalr to the <code>Tensor</code>
constructor, which will not achieve what you want. For
example,<code>torch.Tensor(1)</code> will not give you a <code>Tensor</code> which contains float 1.
Instead, the produced <code>Tensor</code> is something like</p>
<blockquote>
<p>1.00000e-20 *
5.4514
[torch.FloatTensor of size 1]</p>
</blockquote>
<p>To achieve what you want, you have to provide a list with single element 1 to
the <code>Tensor</code> constructor.</p>
<h1 id=references>References</h1>
<ul>
<li><a href=https://github.com/pytorch/pytorch/issues/845>Integer type Tensor only works with integer, but float type Tensor works both with integer and float</a>.</li>
<li><a href=https://discuss.pytorch.org/t/problems-with-weight-array-of-floattensor-type-in-loss-function/381>Tensor types must match</a>.</li>
<li><a href=https://discuss.pytorch.org/t/problems-with-target-arrays-of-int-int32-types-in-loss-functions/140/3>Some loss functions require target to be LongTensor</a>.</li>
<li><a href=https://github.com/pytorch/pytorch/issues/2246>NumPy and torch Tensor conversion, shared memory or not?</a></li>
</ul>
</div>
<div class=post-copyright>
<p class=copyright-item>
<span class=item-title>Author</span>
<span class=item-content>jdhao</span>
</p>
<p class=copyright-item>
<span class=item-title>LastMod</span>
<span class=item-content>
2021-10-16
</span>
</p>
<p class=copyright-item>
<span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span>
</p>
</div>
<div class=post-reward>
<input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>Reward</label>
<div class=qr-code>
<label class=qr-code-image for=reward>
<img class=image src=https://blog-resource-1257868508.file.myqcloud.com/wechat.png>
<span>wechat</span>
</label>
<label class=qr-code-image for=reward>
<img class=image src=https://blog-resource-1257868508.file.myqcloud.com/zhifubao.jpg>
<span>alipay</span>
</label>
</div>
</div><footer class=post-footer>
<div class=post-tags>
<a href=/tags/PyTorch/>PyTorch</a>
</div>
<nav class=post-nav>
<a class=prev href=/2017/11/16/matplotlib-plotting-notes/>
<i class="iconfont icon-left"></i>
<span class="prev-text nav-default">Matplotlib Plotting Notes -- Series 1</span>
<span class="prev-text nav-mobile">Prev</span>
</a>
<a class=next href=/2017/11/12/pytorch-computation-graph/>
<span class="next-text nav-default">Understanding Computational Graphs in PyTorch</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i>
</a>
</nav>
</footer>
</article>
</div>
<script src=https://utteranc.es/client.js repo=jdhao/jdhao.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script>
<noscript>Please enable JavaScript to view the <a href=https://github.com/utterance>comments powered by utterances.</a></noscript>
</div>
</main>
<footer id=footer class=footer>
<div class=social-links>
<a href=mailto:jdhao@hotmail.com class="iconfont icon-email" title=email></a>
<a href="https://stackoverflow.com/users/6064933/jdhao?tab=profile" class="iconfont icon-stack-overflow" title=stack-overflow></a>
<a href=https://www.linkedin.com/in/jiedong-hao-740163227/ class="iconfont icon-linkedin" title=linkedin></a>
<a href=https://github.com/jdhao class="iconfont icon-github" title=github></a>
<a href=https://jdhao.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a>
</div>
<div class=copyright>
<span class=power-by>
Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span>
<span class=division>|</span>
<span class=theme-info>
Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a>
</span>
<div class=busuanzi-footer>
<span id=busuanzi_container_site_pv> site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv> site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
</div>
<span class=copyright-year>
&copy;
2017 -
2022<span class=heart><i class="iconfont icon-heart"></i></span><span>jdhao</span>
</span>
</div>
</footer>
<div class=back-to-top id=back-to-top>
<i class="iconfont icon-up"></i>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams'}}</script>
<script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-113395108-1','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
<script id=baidu_push>(function(){var a,c,b;if(window.location.hostname==='localhost')return;a=document.createElement('script'),a.async=!0,c=window.location.protocol.split(':')[0],c==='https'?a.src='https://zz.bdstatic.com/linksubmit/push.js':a.src='http://push.zhanzhang.baidu.com/push.js',b=document.getElementsByTagName("script")[0],b.parentNode.insertBefore(a,b)})()</script>
</body>
</html>