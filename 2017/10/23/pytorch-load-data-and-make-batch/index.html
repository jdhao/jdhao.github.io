<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Writing Your Own Custom Dataset for Classification in PyTorch - jdhao's blog</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="jdhao"><meta name=description content="In this post, I&amp;rsquo;d like to talk about how to create your own dataset, process it and make data batches ready to be fed into your neural networks, with the help of PyTorch."><meta name=keywords content="Hugo,theme,even"><meta name=google-site-verification content="HTz0VHxqny_b0FfS774dICLBzHGBZCb_S11j_akF1Tw"><meta name=generator content="Hugo 0.75.1 with theme even"><link rel=canonical href=https://jdhao.github.io/2017/10/23/pytorch-load-data-and-make-batch/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script><script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"ca-pub-6058871559165202",enable_page_level_ads:true});</script><link href=/sass/main.min.c7bc1becf36bcf6a9ebd25d2947e43a2eb745ddb0c9a32b43126fd7fa460c351.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="Writing Your Own Custom Dataset for Classification in PyTorch"><meta property="og:description" content="In this post, I&rsquo;d like to talk about how to create your own dataset, process it
and make data batches ready to be fed into your neural networks, with the help
of PyTorch."><meta property="og:type" content="article"><meta property="og:url" content="https://jdhao.github.io/2017/10/23/pytorch-load-data-and-make-batch/"><meta property="article:published_time" content="2017-10-23T17:14:26+08:00"><meta property="article:modified_time" content="2019-07-10T23:41:48+08:00"><meta itemprop=name content="Writing Your Own Custom Dataset for Classification in PyTorch"><meta itemprop=description content="In this post, I&rsquo;d like to talk about how to create your own dataset, process it
and make data batches ready to be fed into your neural networks, with the help
of PyTorch."><meta itemprop=datePublished content="2017-10-23T17:14:26+08:00"><meta itemprop=dateModified content="2019-07-10T23:41:48+08:00"><meta itemprop=wordCount content="711"><meta itemprop=keywords content="PyTorch,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Writing Your Own Custom Dataset for Classification in PyTorch"><meta name=twitter:description content="In this post, I&rsquo;d like to talk about how to create your own dataset, process it
and make data batches ready to be fed into your neural networks, with the help
of PyTorch."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>jdhao's blog</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>jdhao's blog</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>Writing Your Own Custom Dataset for Classification in PyTorch</h1><div class=post-meta><span class=post-time>2017-10-23</span><div class=post-category><a href=/categories/Deep-Learning/>Deep-Learning</a></div><span class=more-meta>711 words</span>
<span class=more-meta>4 mins read</span>
<span id=busuanzi_container_page_pv class=more-meta><span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#create-your-dataset-class>Create your <code>Dataset</code> class</a><ul><li><a href=#overview>Overview</a></li><li><a href=#data-augmentation-and-preprocessing>Data augmentation and preprocessing</a></li></ul></li><li><a href=#create-data-batch-using-dataloader>Create data batch using <code>Dataloader</code></a><ul><li><a href=#loading-variable-size-input-images>Loading variable size input images</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ul></nav></div></div><div class=post-content><p>In this post, I&rsquo;d like to talk about how to create your own dataset, process it
and make data batches ready to be fed into your neural networks, with the help
of PyTorch.</p><p>In PyTorch, in order to feed your own training data into the network, you will
mainly deal with two classes: the
<a href=http://pytorch.org/docs/master/data.html#torch.utils.data.Dataset>Dataset</a>
class and the
<a href=http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader>Dataloader</a>
class. Now I will explain in more detail what they do.</p><h1 id=create-your-dataset-class>Create your <code>Dataset</code> class</h1><h2 id=overview>Overview</h2><p><code>Dataset</code> class is used to provide an interface for accessing all the training
or testing samples in your dataset. In order to achieve this, you have to
implement at least two methods, <code>__getitem__</code> and <code>__len__</code> so that each
training sample (in image classification, a sample means an image plus its
class label) can be accessed by its index.</p><p>In the initialization part of the class, you should collect a list of all the
images and its labels in the dataset. When we want to get a particular sample,
we then read the image, transform it and return the transformed image and the
corresponding label.</p><p>A good example is <code>ImageFolder</code> class provided by <code>torchvision</code> package, you
can check its source code
<a href=https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py>here</a>
to get a sense of how it actually works.</p><h2 id=data-augmentation-and-preprocessing>Data augmentation and preprocessing</h2><p>Data augmentation and preprocessing is an important part of the whole
work-flow. In PyTorch, we do it by providing a <code>transform</code> parameter to the
<code>Dataset</code> class. <code>Transform</code> are class object which are called to process the
given input. You can cascade a series of transforms by providing a list of
transforms to
<a href=http://pytorch.org/docs/master/torchvision/transforms.html#torchvision.transforms.Compose><code>torchvision.transforms.Compose</code></a>
method. Then the given transforms will be performed on the input in the order
they appear.</p><p>It should be noted that some of the transforms are for <code>PIL</code> image object, such
as
<a href=http://pytorch.org/docs/master/torchvision/transforms.html#torchvision.transforms.RandomCrop>RandomCrop()</a>
and
<a href=http://pytorch.org/docs/master/torchvision/transforms.html#torchvision.transforms.Resize>Resize()</a>.
Other transforms are for torch <code>Tensor</code>, such as
<a href=http://pytorch.org/docs/master/torchvision/transforms.html#torchvision.transforms.Normalize>Normalize</a>.
If your dataset contains images, you should first perform all transforms
expecting <code>PIL</code> image object, then convert <code>PIL</code> image to <code>Tensor</code> using
<a href=http://pytorch.org/docs/master/torchvision/transforms.html#torchvision.transforms.ToTensor>ToTensor()</a>
method. The <code>ToTensor</code> transform will convert <code>PIL</code> image to torch <code>Tensor</code> of
shape $H\times W\times C$, with its values in the range [0.0, 1.0].</p><p>The <code>Normalize</code> transform expects torch tensors. Its parameters are the means
and standard deviations of RGB channels of all the training images. For
ImageNet, the devs have already done that for us, the normalize transform
should be</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>normalize</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>(</span><span class=n>mean</span><span class=o>=</span><span class=p>[</span><span class=mf>0.485</span><span class=p>,</span> <span class=mf>0.456</span><span class=p>,</span> <span class=mf>0.406</span><span class=p>],</span>
                                 <span class=n>std</span><span class=o>=</span><span class=p>[</span><span class=mf>0.229</span><span class=p>,</span> <span class=mf>0.224</span><span class=p>,</span> <span class=mf>0.225</span><span class=p>])</span>
</code></pre></div><p>For your own dataset, you have to calculate the statistics yourself.</p><h1 id=create-data-batch-using-dataloader>Create data batch using <code>Dataloader</code></h1><p>Although we can access all the training data using the <code>Dataset</code> class, but
that is not enough. For deep learning, we need the functionality such as
batching, shuffling, multiprocess data loading, etc. This is what the
<code>Dataloader</code> class do.</p><p>The <code>Dataloader</code> class accept a dataset and other parameters such as
<code>batch_size</code>, <code>batch_sampler</code> and number of <code>workers</code> to load the data and so
on&mldr; Then we can iterate over the <code>Dataloader</code> to get batches of training data
and train our models.</p><h2 id=loading-variable-size-input-images>Loading variable size input images</h2><p>By default, <code>Dataloader</code> use
<a href=https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L83><code>collate_fn</code></a>
method to pack a series of images and target as tensors (first dimension of
tensor is batch size). The default <code>collate_fn</code> expects all the images in a
batch to have the same size because it uses <code>torch.stack()</code> to pack the images.
If the images provided by <code>Dataset</code> have variable size, you have to provide
your custom <code>collate_fn</code>. A simple example is shown below:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=c1># a simple custom collate function, just to show the idea</span>

<span class=c1># `batch` is a list of tuple where first element is image tensor and</span>

<span class=c1># second element is corresponding label</span>

<span class=k>def</span> <span class=nf>my_collate</span><span class=p>(</span><span class=n>batch</span><span class=p>):</span>
    <span class=n>data</span> <span class=o>=</span> <span class=p>[</span><span class=n>item</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=k>for</span> <span class=n>item</span> <span class=ow>in</span> <span class=n>batch</span><span class=p>]</span>  <span class=c1># just form a list of tensor</span>

    <span class=n>target</span> <span class=o>=</span> <span class=p>[</span><span class=n>item</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=k>for</span> <span class=n>item</span> <span class=ow>in</span> <span class=n>batch</span><span class=p>]</span>
    <span class=n>target</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>(</span><span class=n>target</span><span class=p>)</span>
    <span class=k>return</span> <span class=p>[</span><span class=n>data</span><span class=p>,</span> <span class=n>target</span><span class=p>]</span>
</code></pre></div><h1 id=conclusion>Conclusion</h1><p>In this post, I give an introduction to the use of <code>Dataset</code> and <code>Dataloader</code>
in PyTorch. <code>Dataset</code> is used to access single sample from your dataset and
transform it, while <code>Dataloader</code> is used to load a batch of samples for
training or testing your models. If your training images have variable size,
you may also have to use your own custom <code>collate_fn</code>.</p><h1 id=references>References</h1><ul><li><a href=https://discuss.pytorch.org/t/how-to-preprocess-input-for-pre-trained-networks/683/3>Normalization for input images</a>.</li><li><a href=https://discuss.pytorch.org/t/whats-the-range-of-the-input-value-desired-to-use-pretrained-resnet152-and-vgg19/1683>Another post about image normalization</a>.</li><li><a href=https://discuss.pytorch.org/t/how-to-create-a-dataloader-with-variable-size-input/8278/3>Dataloader with variable size images</a>.</li><li><a href=https://discuss.pytorch.org/t/how-to-create-convnet-for-variable-size-input-dimension-images/1906>ConvNet for variable size input image</a>.</li></ul></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>jdhao</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2019-07-10</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>Reward</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=https://blog-resource-1257868508.file.myqcloud.com/wechat.png>
<span>wechat</span></label>
<label class=qr-code-image for=reward><img class=image src=https://blog-resource-1257868508.file.myqcloud.com/zhifubao.jpg>
<span>alipay</span></label></div></div><footer class=post-footer><div class=post-tags><a href=/tags/PyTorch/>PyTorch</a></div><nav class=post-nav><a class=prev href=/2017/10/24/the-stackoverflow-love-and-hatred/><i class="iconfont icon-left"></i><span class="prev-text nav-default">对 Stack Overflow 的爱与恨</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=/2017/10/21/useful-sublime-shortcut-and-plugin/><span class="next-text nav-default">Sublime Text Tips and Tricks That Will Make Your Life Easier</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><div id=disqus_thread></div><script type=text/javascript>(function(){if(window.location.hostname==='localhost')return;var dsq=document.createElement('script');dsq.type='text/javascript';dsq.async=true;var disqus_shortname='jdhao';dsq.src='//'+disqus_shortname+'.disqus.com/embed.js';(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(dsq);})();</script><noscript>Please enable JavaScript to view the <a href=http://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></main><footer id=footer class=footer><div class=social-links><a href="https://stackoverflow.com/users/6064933/jdhao?tab=profile" class="iconfont icon-stack-overflow" title=stack-overflow></a><a href=https://github.com/jdhao class="iconfont icon-github" title=github></a><a href=https://jdhao.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span></span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv>site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span></span></div><span class=copyright-year>&copy;
2017 -
2020<span class=heart><i class="iconfont icon-heart"></i></span><span>jdhao</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js></script><script type=text/javascript>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams',}};</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-113395108-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script id=baidu_push>(function(){if(window.location.hostname==='localhost')return;var bp=document.createElement('script');bp.async=true;var curProtocol=window.location.protocol.split(':')[0];if(curProtocol==='https'){bp.src='https://zz.bdstatic.com/linksubmit/push.js';}
else{bp.src='http://push.zhanzhang.baidu.com/push.js';}
var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script></body></html>