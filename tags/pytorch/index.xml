<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>PyTorch on jdhao's digital space</title><link>https://jdhao.github.io/tags/pytorch/</link><description>Recent content in PyTorch on jdhao's digital space</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2017 - 2026 ❤️jdhao</copyright><lastBuildDate>Wed, 04 May 2022 06:57:41 +0200</lastBuildDate><atom:link href="https://jdhao.github.io/tags/pytorch/index.xml" rel="self" type="application/rss+xml"/><item><title>How to Use Tensorboard in Pytorch</title><link>https://jdhao.github.io/2022/04/20/pytorch-tensorboard-use/</link><pubDate>Wed, 20 Apr 2022 20:53:51 +0800</pubDate><guid>https://jdhao.github.io/2022/04/20/pytorch-tensorboard-use/</guid><description>&lt;p&gt;This is a brief note on how to use &lt;a href="https://github.com/tensorflow/tensorboard" target="_blank" rel="noreferrer"&gt;Tensorboard&lt;/a&gt; in PyTorch.&lt;/p&gt;</description></item><item><title>Accelerate Batched Image Inference in PyTorch</title><link>https://jdhao.github.io/2022/03/18/torch_accelerate_batch_inference/</link><pubDate>Fri, 18 Mar 2022 22:59:33 +0800</pubDate><guid>https://jdhao.github.io/2022/03/18/torch_accelerate_batch_inference/</guid><description>&lt;p&gt;I have a web service where the images come in a batch so I have to do inference for several images in PIL format at a time.
Initially, I use a naive approach and just transform the images one by one,
then combine them to form a single tensor and do the inference.
The inference becomes really slow when I have a batch with more than 30 images.
The inference time is about 1-2 seconds per batch.&lt;/p&gt;</description></item><item><title>Dependency Hell When Building A PyTorch GPU Docker Image</title><link>https://jdhao.github.io/2022/02/09/dependency-hell-build-torch-gpu-docker-container/</link><pubDate>Wed, 09 Feb 2022 21:30:00 +0800</pubDate><guid>https://jdhao.github.io/2022/02/09/dependency-hell-build-torch-gpu-docker-container/</guid><description>&lt;p&gt;In order to for PyTorch to use host GPU inside a Docker container, their versions must match.&lt;/p&gt;</description></item><item><title>A Dig into PyTorch Model Loading</title><link>https://jdhao.github.io/2022/01/28/pytorch_model_load_error/</link><pubDate>Fri, 28 Jan 2022 23:17:45 +0800</pubDate><guid>https://jdhao.github.io/2022/01/28/pytorch_model_load_error/</guid><description>&lt;h1 class="relative group"&gt;Saving and loading PyTorch models
 &lt;div id="saving-and-loading-pytorch-models" class="anchor"&gt;&lt;/div&gt;
 
 &lt;span
 class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"&gt;
 &lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#saving-and-loading-pytorch-models" aria-label="Anchor"&gt;#&lt;/a&gt;
 &lt;/span&gt;
 
&lt;/h1&gt;
&lt;p&gt;Models in PyTorch are a subclass of &lt;code&gt;torch.nn.Module&lt;/code&gt;. To save the model parameters,
we use &lt;code&gt;model.state_dict()&lt;/code&gt; to get all the model parameters:&lt;/p&gt;</description></item><item><title>Set the Number of Threads to Use in PyTorch</title><link>https://jdhao.github.io/2020/07/06/pytorch_set_num_threads/</link><pubDate>Mon, 06 Jul 2020 23:15:25 +0800</pubDate><guid>https://jdhao.github.io/2020/07/06/pytorch_set_num_threads/</guid><description>&lt;p&gt;In this post, I will share how PyTorch set the number of the threads to use for
its operations.&lt;/p&gt;</description></item><item><title>Distributed Training in PyTorch with Horovod</title><link>https://jdhao.github.io/2019/11/01/pytorch_distributed_training/</link><pubDate>Fri, 01 Nov 2019 22:26:53 +0800</pubDate><guid>https://jdhao.github.io/2019/11/01/pytorch_distributed_training/</guid><description>&lt;p&gt;&lt;a href="https://github.com/horovod/horovod" target="_blank" rel="noreferrer"&gt;Horovod&lt;/a&gt; is the distributed training
framework developed by Uber. It support training distributed programs with
little modification for both TensorFlow, PyTorch, MXNet and keras.&lt;/p&gt;</description></item><item><title>Difference between view, reshape, transpose and permute in PyTorch</title><link>https://jdhao.github.io/2019/07/10/pytorch_view_reshape_transpose_permute/</link><pubDate>Wed, 10 Jul 2019 23:21:48 +0800</pubDate><guid>https://jdhao.github.io/2019/07/10/pytorch_view_reshape_transpose_permute/</guid><description>&lt;p&gt;PyTorch provides a lot of methods for the Tensor type. Some of these methods
may be confusing for new users. Here, I would like to talk about
&lt;a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view" target="_blank" rel="noreferrer"&gt;&lt;code&gt;view()&lt;/code&gt;&lt;/a&gt; vs
&lt;a href="https://pytorch.org/docs/stable/torch.html#torch.reshape" target="_blank" rel="noreferrer"&gt;&lt;code&gt;reshape()&lt;/code&gt;&lt;/a&gt;,
&lt;a href="https://pytorch.org/docs/stable/torch.html#torch.transpose" target="_blank" rel="noreferrer"&gt;&lt;code&gt;transpose()&lt;/code&gt;&lt;/a&gt; vs
&lt;a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute" target="_blank" rel="noreferrer"&gt;&lt;code&gt;permute()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;</description></item><item><title>Set Default GPU in PyTorch</title><link>https://jdhao.github.io/2018/04/02/pytorch-gpu-usage/</link><pubDate>Mon, 02 Apr 2018 11:13:22 +0800</pubDate><guid>https://jdhao.github.io/2018/04/02/pytorch-gpu-usage/</guid><description>&lt;p&gt;You can use two ways to set the GPU you want to use by default.&lt;/p&gt;</description></item><item><title>Notes on PyTorch Tensor Data Types</title><link>https://jdhao.github.io/2017/11/15/pytorch-datatype-note/</link><pubDate>Wed, 15 Nov 2017 21:22:45 +0800</pubDate><guid>https://jdhao.github.io/2017/11/15/pytorch-datatype-note/</guid><description>&lt;p&gt;In PyTorch,
&lt;a href="http://pytorch.org/docs/master/tensors.html#torch-tensor" target="_blank" rel="noreferrer"&gt;&lt;code&gt;Tensor&lt;/code&gt;&lt;/a&gt; is the
primary object that we deal with (&lt;code&gt;Variable&lt;/code&gt; is just a thin wrapper class for
&lt;code&gt;Tensor&lt;/code&gt;). In this post, I will give a summary of pitfalls that we should avoid
when using Tensors. Since &lt;code&gt;FloatTensor&lt;/code&gt; and &lt;code&gt;LongTensor&lt;/code&gt; are the most popular
&lt;code&gt;Tensor&lt;/code&gt; types in PyTorch, I will focus on these two data types.&lt;/p&gt;</description></item><item><title>Understanding Computational Graphs in PyTorch</title><link>https://jdhao.github.io/2017/11/12/pytorch-computation-graph/</link><pubDate>Sun, 12 Nov 2017 13:22:46 +0800</pubDate><guid>https://jdhao.github.io/2017/11/12/pytorch-computation-graph/</guid><description>&lt;p&gt;PyTorch is a relatively new deep learning library which support dynamic
computation graphs. It has gained a lot of attention after its official release
in January. In this post, I want to share what I have learned about the
computation graph in PyTorch. Without basic knowledge of computation graph, we
can hardly understand what is actually happening under the hood when we are
trying to train our &lt;em&gt;landscape-changing&lt;/em&gt; neural networks.&lt;/p&gt;</description></item><item><title>Writing Your Own Custom Dataset for Classification in PyTorch</title><link>https://jdhao.github.io/2017/10/23/pytorch-load-data-and-make-batch/</link><pubDate>Mon, 23 Oct 2017 17:14:26 +0800</pubDate><guid>https://jdhao.github.io/2017/10/23/pytorch-load-data-and-make-batch/</guid><description>&lt;p&gt;In this post, I&amp;rsquo;d like to talk about how to create your own dataset, process it
and make data batches ready to be fed into your neural networks, with the help
of PyTorch.&lt;/p&gt;</description></item></channel></rss>