<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CNN on jdhao's digital space</title><link>https://jdhao.github.io/tags/cnn/</link><description>Recent content in CNN on jdhao's digital space</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2017 - 2025 ❤️ jdhao</copyright><lastBuildDate>Fri, 29 Sep 2017 09:40:00 +0800</lastBuildDate><atom:link href="https://jdhao.github.io/tags/cnn/index.xml" rel="self" type="application/rss+xml"/><item><title>1x1 Convolutions Demystified</title><link>https://jdhao.github.io/2017/09/29/1by1-convolution-in-cnn/</link><pubDate>Fri, 29 Sep 2017 09:40:00 +0800</pubDate><guid>https://jdhao.github.io/2017/09/29/1by1-convolution-in-cnn/</guid><description>&lt;p>In the early development of convolutional neural networks (CNNs),
convolutions with kernel size &lt;span class="math inline">\(3\times
3\)&lt;/span>, &lt;span class="math inline">\(5\times 5\)&lt;/span>, &lt;span
class="math inline">\(7\times 7\)&lt;/span> or even &lt;span
class="math inline">\(11\times 11\)&lt;/span> are often used. In the more
recent literature, however, &lt;span class="math inline">\(1\times
1\)&lt;/span> convolutions are becoming prevalent. In this post, I will try
to explain what &lt;span class="math inline">\(1\times 1\)&lt;/span>
convolutions are and discuss why they are used in CNNs.&lt;/p></description></item><item><title>神经网络中误差反向传播(back propagation)算法的工作原理</title><link>https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/</link><pubDate>Tue, 19 Jan 2016 00:00:00 +0800</pubDate><guid>https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/</guid><description>&lt;p>神经网络，从大学时期就听说过，后面上课的时候老师也讲过，但是感觉从来没有真正掌握，总是似是而非，比较模糊，好像懂，其实并不懂。&lt;/p>
&lt;p>为了搞懂神经网络的运行原理，有必要搞清楚神经网络最核心的算法，也就是&lt;a
href="https://en.wikipedia.org/wiki/Backpropagation">误差反向传播算法&lt;/a>的工作原理，本文以最简单的全连接神经网络为例，介绍误差反向传播算法的原理。&lt;/p></description></item></channel></rss>