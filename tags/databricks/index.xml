<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Databricks on jdhao's digital space</title><link>https://jdhao.github.io/tags/databricks/</link><description>Recent content in Databricks on jdhao's digital space</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2017 - 2025 ❤️ jdhao</copyright><lastBuildDate>Tue, 21 Nov 2023 23:56:36 +0100</lastBuildDate><atom:link href="https://jdhao.github.io/tags/databricks/index.xml" rel="self" type="application/rss+xml"/><item><title>How to Download Files from Google Cloud Storage in the Databricks Workspace Notebook</title><link>https://jdhao.github.io/2023/11/21/databricks-notebook-gcloud-storage-download/</link><pubDate>Tue, 21 Nov 2023 23:56:36 +0100</pubDate><guid>https://jdhao.github.io/2023/11/21/databricks-notebook-gcloud-storage-download/</guid><description>&lt;p>In this post, I want to share the complete process and setup to download files from GCP in a Databricks workspace notebook.
Since the notebook itself is non-interactive when you run the shell command,
the setup process is a bit different from the normal GCP authentication.&lt;/p></description></item><item><title>Databricks Cli Usage</title><link>https://jdhao.github.io/2023/11/21/databricks-cli-usage/</link><pubDate>Tue, 21 Nov 2023 23:49:06 +0100</pubDate><guid>https://jdhao.github.io/2023/11/21/databricks-cli-usage/</guid><description>&lt;p>We can use the &lt;a href="https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/" target="_blank">Databricks cli&lt;/a> to interact with Databricks workspaces programmatically on the command line.
In this post, I want to share simple usage about Databricks cli.&lt;/p></description></item><item><title>Working with Databricks Workspace Files</title><link>https://jdhao.github.io/2023/11/18/databricks-workspace-files/</link><pubDate>Sat, 18 Nov 2023 22:34:20 +0100</pubDate><guid>https://jdhao.github.io/2023/11/18/databricks-workspace-files/</guid><description>&lt;p>Some observation and finding in working with Databricks workspace files.&lt;/p></description></item><item><title>Databricks Init Scripts</title><link>https://jdhao.github.io/2023/10/21/databricks-init-scripts/</link><pubDate>Sat, 21 Oct 2023 00:54:33 +0200</pubDate><guid>https://jdhao.github.io/2023/10/21/databricks-init-scripts/</guid><description>General # Init script is just a shell script, which will be run for each node in the cluster, before Apache Spark driver or executor JVM starts.
A cluster can have multiple init script if you want. These init scripts will be executed in the order provided.
Cluster scope init script # If your cluster is not in Edit mode, you can not see the button to add init script. You need to click Edit in the cluster configuration page, then you can add init script to your cluster settings.</description></item><item><title>File Systems in Databricks</title><link>https://jdhao.github.io/2023/10/03/databricks-dbfs-and-other-filesystem/</link><pubDate>Tue, 03 Oct 2023 15:35:28 +0200</pubDate><guid>https://jdhao.github.io/2023/10/03/databricks-dbfs-and-other-filesystem/</guid><description>&lt;p>A summary of different file systems in Databricks.&lt;/p></description></item><item><title>Change Timezone in Databricks Spark</title><link>https://jdhao.github.io/2023/09/15/databricks-spark-change-timezone/</link><pubDate>Fri, 15 Sep 2023 20:12:22 +0200</pubDate><guid>https://jdhao.github.io/2023/09/15/databricks-spark-change-timezone/</guid><description>&lt;p>The Databricks cluster is using UTC as the default timezone.
So when you run some time-related code, the displayed time is not the local time, which is not ideal.
In this post, I want to share how to change the timezone setting for Databricks cluster.&lt;/p></description></item><item><title>Running/importing Python code/module in Databricks</title><link>https://jdhao.github.io/2023/06/06/databricks-run-import-code/</link><pubDate>Tue, 06 Jun 2023 21:37:45 +0200</pubDate><guid>https://jdhao.github.io/2023/06/06/databricks-run-import-code/</guid><description>&lt;p>Databricks is centered around notebooks for doing a lot of works.
As a result, it is often not obvious how to run or re-use code from another notebook or Python module.
In this post, I want to share how to do this in the Databricks.&lt;/p></description></item><item><title>How to get or set Databricks spark configuration</title><link>https://jdhao.github.io/2023/05/13/databricks-spark-get-set-conf/</link><pubDate>Sat, 13 May 2023 21:57:26 +0200</pubDate><guid>https://jdhao.github.io/2023/05/13/databricks-spark-get-set-conf/</guid><description>&lt;p>In this post, I summarize how to get or set a Databricks spark configuration/property.&lt;/p></description></item></channel></rss>