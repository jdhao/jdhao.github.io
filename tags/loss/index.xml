<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Loss on Blowfish</title><link>https://jdhao.github.io/tags/loss/</link><description>Recent content in Loss on Blowfish</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2017 - 2024 ❤️ jdhao</copyright><lastBuildDate>Sat, 16 Oct 2021 09:54:49 +0800</lastBuildDate><atom:link href="https://jdhao.github.io/tags/loss/index.xml" rel="self" type="application/rss+xml"/><item><title>Why Use Cross Entropy in Classification Task?</title><link>https://jdhao.github.io/2021/10/16/why_cross_entropy_in_classification/</link><pubDate>Sat, 16 Oct 2021 09:54:49 +0800</pubDate><guid>https://jdhao.github.io/2021/10/16/why_cross_entropy_in_classification/</guid><description>&lt;p>In classification tasks, the de facto loss to use is the &lt;a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" target="_blank">
cross entropy loss&lt;/a>.&lt;/p></description></item><item><title>Some Loss Functions and Their Intuitive Explanations</title><link>https://jdhao.github.io/2017/03/13/some_loss_and_explanations/</link><pubDate>Mon, 13 Mar 2017 10:14:55 +0800</pubDate><guid>https://jdhao.github.io/2017/03/13/some_loss_and_explanations/</guid><description>&lt;p>Loss functions are frequently used in supervised machine learning to
minimize the differences between the predicted output of the model and
the ground truth labels. In other words, it is used to measure how good
our model can predict the true class of a sample from the dataset. Here
I would like to list some frequently-used loss functions and give my
intuitive explanation.&lt;/p></description></item></channel></rss>