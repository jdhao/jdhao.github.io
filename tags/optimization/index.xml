<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Optimization on jdhao's digital space</title><link>https://jdhao.github.io/tags/optimization/</link><description>Recent content in Optimization on jdhao's digital space</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2017 - 2025 ❤️ jdhao</copyright><lastBuildDate>Thu, 28 Apr 2022 21:45:01 +0800</lastBuildDate><atom:link href="https://jdhao.github.io/tags/optimization/index.xml" rel="self" type="application/rss+xml"/><item><title>How to Calculate Square Root without Using sqrt()?</title><link>https://jdhao.github.io/2022/04/28/sqrt_without_using_builtin_sqrt/</link><pubDate>Thu, 28 Apr 2022 21:45:01 +0800</pubDate><guid>https://jdhao.github.io/2022/04/28/sqrt_without_using_builtin_sqrt/</guid><description>&lt;p>I saw an interesting question that how to get square root of x
without using builtin function from your language. There are different
ways to approach this problem.&lt;/p></description></item><item><title>Understanding Computational Graphs in PyTorch</title><link>https://jdhao.github.io/2017/11/12/pytorch-computation-graph/</link><pubDate>Sun, 12 Nov 2017 13:22:46 +0800</pubDate><guid>https://jdhao.github.io/2017/11/12/pytorch-computation-graph/</guid><description>&lt;p>PyTorch is a relatively new deep learning library which support dynamic
computation graphs. It has gained a lot of attention after its official release
in January. In this post, I want to share what I have learned about the
computation graph in PyTorch. Without basic knowledge of computation graph, we
can hardly understand what is actually happening under the hood when we are
trying to train our &lt;em>landscape-changing&lt;/em> neural networks.&lt;/p></description></item><item><title>关于神经网络优化的一些思考</title><link>https://jdhao.github.io/2016/11/25/thought-on-optimization-for-cnn/</link><pubDate>Fri, 25 Nov 2016 00:00:00 +0000</pubDate><guid>https://jdhao.github.io/2016/11/25/thought-on-optimization-for-cnn/</guid><description>&lt;p>优化的方法有很多种，在深度学习中，占有绝对主导地位的还是 stochastic gradient
optimization (简称 SGD) 以及它的一些变种，如 SGD with momentum，Adam 等。 SGD
是一种基于一阶梯度信息的优化方法，仅从优化的速度上来讲，效率不是最高的，一些利
用二阶信息的优化方法，理论上优化速度更快，但是，SGD 反而是在深度学习的优化中使
用的最多的优化方法，为什么其他类型的优化方法在深度学习中不经常使用呢？它们相比
SGD 有什么缺点？或者说 SGD 有什么优点呢？ 这篇文章试图对这个问题给出自己的思考
与总结。&lt;/p></description></item><item><title>神经网络中误差反向传播(back propagation)算法的工作原理</title><link>https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/</link><pubDate>Tue, 19 Jan 2016 00:00:00 +0800</pubDate><guid>https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/</guid><description>&lt;p>神经网络，从大学时期就听说过，后面上课的时候老师也讲过，但是感觉从来没有真正掌握，总是似是而非，比较模糊，好像懂，其实并不懂。&lt;/p>
&lt;p>为了搞懂神经网络的运行原理，有必要搞清楚神经网络最核心的算法，也就是&lt;a
href="https://en.wikipedia.org/wiki/Backpropagation">误差反向传播算法&lt;/a>的工作原理，本文以最简单的全连接神经网络为例，介绍误差反向传播算法的原理。&lt;/p></description></item></channel></rss>