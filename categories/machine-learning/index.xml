<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine-Learning on jdhao's digital space</title><link>https://jdhao.github.io/categories/machine-learning/</link><description>Recent content in Machine-Learning on jdhao's digital space</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2017 - 2024 ❤️ jdhao</copyright><lastBuildDate>Sun, 27 Mar 2022 17:25:12 +0800</lastBuildDate><atom:link href="https://jdhao.github.io/categories/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Nonlinear Activations for Neural Networks</title><link>https://jdhao.github.io/2022/03/27/neural-network-nonlinear-activation/</link><pubDate>Sun, 27 Mar 2022 17:25:12 +0800</pubDate><guid>https://jdhao.github.io/2022/03/27/neural-network-nonlinear-activation/</guid><description>&lt;p align="center">
&lt;img src="https://blog-resource-1257868508.file.myqcloud.com/202203271740705.png" width="600">
&lt;/p>
&lt;p>Non-linear activations are important in deep neural networks. It is
important in the sense that without non-linear activation functions,
even if you have many linear layers, the end results is like you have
only one linear layer, and the approximation ability of the network is
very limited&lt;a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref">&lt;sup>1&lt;/sup>&lt;/a>. Some of most commonly-used
nonlinear activation functions are Sigmoid, ReLU and Tanh.&lt;/p></description></item><item><title>Accelerate Batched Image Inference in PyTorch</title><link>https://jdhao.github.io/2022/03/18/torch_accelerate_batch_inference/</link><pubDate>Fri, 18 Mar 2022 22:59:33 +0800</pubDate><guid>https://jdhao.github.io/2022/03/18/torch_accelerate_batch_inference/</guid><description>&lt;p>I have a web service where the images come in a batch so I have to do inference for several images in PIL format at a time.
Initially, I use a naive approach and just transform the images one by one,
then combine them to form a single tensor and do the inference.
The inference becomes really slow when I have a batch with more than 30 images.
The inference time is about 1-2 seconds per batch.&lt;/p></description></item><item><title>Softmax with Temperature Explained</title><link>https://jdhao.github.io/2022/02/27/temperature_in_softmax/</link><pubDate>Sun, 27 Feb 2022 12:47:02 +0800</pubDate><guid>https://jdhao.github.io/2022/02/27/temperature_in_softmax/</guid><description>&lt;p align="center">
&lt;img src="https://blog-resource-1257868508.file.myqcloud.com/202202271549378.jpg" width="800">
&lt;/p>
&lt;p>Softmax function is commonly used in classification tasks. Suppose
that we have an input vector &lt;span class="math inline">\([z_1, z_2,
\ldots, z_N]\)&lt;/span>, after softmax, each element becomes:&lt;/p></description></item><item><title>Dependency Hell When Building A PyTorch GPU Docker Image</title><link>https://jdhao.github.io/2022/02/09/dependency-hell-build-torch-gpu-docker-container/</link><pubDate>Wed, 09 Feb 2022 21:30:00 +0800</pubDate><guid>https://jdhao.github.io/2022/02/09/dependency-hell-build-torch-gpu-docker-container/</guid><description>&lt;p>In order to for PyTorch to use host GPU inside a Docker container, their versions must match.&lt;/p></description></item><item><title>A Dig into PyTorch Model Loading</title><link>https://jdhao.github.io/2022/01/28/pytorch_model_load_error/</link><pubDate>Fri, 28 Jan 2022 23:17:45 +0800</pubDate><guid>https://jdhao.github.io/2022/01/28/pytorch_model_load_error/</guid><description>&lt;h1 class="relative group">Saving and loading PyTorch models
&lt;div id="saving-and-loading-pytorch-models" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#saving-and-loading-pytorch-models" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h1>
&lt;p>Models in PyTorch are a subclass of &lt;code>torch.nn.Module&lt;/code>. To save the model parameters,
we use &lt;code>model.state_dict()&lt;/code> to get all the model parameters:&lt;/p></description></item><item><title>Why do We Use LogSumExp in Machine Learning?</title><link>https://jdhao.github.io/2022/01/09/log_sum_exp_in_machine_learning/</link><pubDate>Sun, 09 Jan 2022 20:39:52 +0800</pubDate><guid>https://jdhao.github.io/2022/01/09/log_sum_exp_in_machine_learning/</guid><description>&lt;p align="center">
&lt;img src="https://blog-resource-1257868508.file.myqcloud.com/202201092324184.jpg" width="600">
&lt;/p>
&lt;p>LogSumExp is often used in machine learning. It has the following
form:&lt;/p></description></item><item><title>The Warmup Trick for Training Deep Neural Networks</title><link>https://jdhao.github.io/2020/08/14/warmup_maskrcnn_how_does_it_work/</link><pubDate>Fri, 14 Aug 2020 23:04:54 +0800</pubDate><guid>https://jdhao.github.io/2020/08/14/warmup_maskrcnn_how_does_it_work/</guid><description>&lt;p>Warmup is a training technique often used in training deep neural networks.
In this post, I will try to explain what is warmup, and how does it work.&lt;/p></description></item><item><title>Set the Number of Threads to Use in PyTorch</title><link>https://jdhao.github.io/2020/07/06/pytorch_set_num_threads/</link><pubDate>Mon, 06 Jul 2020 23:15:25 +0800</pubDate><guid>https://jdhao.github.io/2020/07/06/pytorch_set_num_threads/</guid><description>&lt;p>In this post, I will share how PyTorch set the number of the threads to use for
its operations.&lt;/p></description></item><item><title>Labelme JSON 标注格式转 voc XML 格式</title><link>https://jdhao.github.io/2019/12/21/labelme_json_to_voc_xml/</link><pubDate>Sat, 21 Dec 2019 23:37:49 +0800</pubDate><guid>https://jdhao.github.io/2019/12/21/labelme_json_to_voc_xml/</guid><description>&lt;p>Labelme 是一款常用的计算机视觉任务标注工具，可以用来标注分类，检测，分割等任务的数据。对于检测任务，labelme 生成的标注文件是 json 格式，每个图像对应一个相应的 json 文件。但是很多任务都使用 PASCAL VOC 的 xml 格式标注，例如 &lt;a href="https://github.com/facebookresearch/maskrcnn-benchmark" target="_blank">maskrcnn-benchmark&lt;/a> 任务中的 &lt;a href="https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/data/datasets/voc.py" target="_blank">voc 数据集&lt;/a>。&lt;/p></description></item><item><title>Distributed Training in PyTorch with Horovod</title><link>https://jdhao.github.io/2019/11/01/pytorch_distributed_training/</link><pubDate>Fri, 01 Nov 2019 22:26:53 +0800</pubDate><guid>https://jdhao.github.io/2019/11/01/pytorch_distributed_training/</guid><description>&lt;p>&lt;a href="https://github.com/horovod/horovod" target="_blank">Horovod&lt;/a> is the distributed training
framework developed by Uber. It support training distributed programs with
little modification for both TensorFlow, PyTorch, MXNet and keras.&lt;/p></description></item><item><title>Difference between view, reshape, transpose and permute in PyTorch</title><link>https://jdhao.github.io/2019/07/10/pytorch_view_reshape_transpose_permute/</link><pubDate>Wed, 10 Jul 2019 23:21:48 +0800</pubDate><guid>https://jdhao.github.io/2019/07/10/pytorch_view_reshape_transpose_permute/</guid><description>&lt;p>PyTorch provides a lot of methods for the Tensor type. Some of these methods
may be confusing for new users. Here, I would like to talk about
&lt;a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view" target="_blank">&lt;code>view()&lt;/code>&lt;/a> vs
&lt;a href="https://pytorch.org/docs/stable/torch.html#torch.reshape" target="_blank">&lt;code>reshape()&lt;/code>&lt;/a>,
&lt;a href="https://pytorch.org/docs/stable/torch.html#torch.transpose" target="_blank">&lt;code>transpose()&lt;/code>&lt;/a> vs
&lt;a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute" target="_blank">&lt;code>permute()&lt;/code>&lt;/a>.&lt;/p></description></item><item><title>Set Default GPU in PyTorch</title><link>https://jdhao.github.io/2018/04/02/pytorch-gpu-usage/</link><pubDate>Mon, 02 Apr 2018 11:13:22 +0800</pubDate><guid>https://jdhao.github.io/2018/04/02/pytorch-gpu-usage/</guid><description>&lt;p>You can use two ways to set the GPU you want to use by default.&lt;/p></description></item><item><title>Understanding Computational Graphs in PyTorch</title><link>https://jdhao.github.io/2017/11/12/pytorch-computation-graph/</link><pubDate>Sun, 12 Nov 2017 13:22:46 +0800</pubDate><guid>https://jdhao.github.io/2017/11/12/pytorch-computation-graph/</guid><description>&lt;p>PyTorch is a relatively new deep learning library which support dynamic
computation graphs. It has gained a lot of attention after its official release
in January. In this post, I want to share what I have learned about the
computation graph in PyTorch. Without basic knowledge of computation graph, we
can hardly understand what is actually happening under the hood when we are
trying to train our &lt;em>landscape-changing&lt;/em> neural networks.&lt;/p></description></item><item><title>Writing Your Own Custom Dataset for Classification in PyTorch</title><link>https://jdhao.github.io/2017/10/23/pytorch-load-data-and-make-batch/</link><pubDate>Mon, 23 Oct 2017 17:14:26 +0800</pubDate><guid>https://jdhao.github.io/2017/10/23/pytorch-load-data-and-make-batch/</guid><description>&lt;p>In this post, I&amp;rsquo;d like to talk about how to create your own dataset, process it
and make data batches ready to be fed into your neural networks, with the help
of PyTorch.&lt;/p></description></item><item><title>1x1 Convolutions Demystified</title><link>https://jdhao.github.io/2017/09/29/1by1-convolution-in-cnn/</link><pubDate>Fri, 29 Sep 2017 09:40:00 +0800</pubDate><guid>https://jdhao.github.io/2017/09/29/1by1-convolution-in-cnn/</guid><description>&lt;p>In the early development of convolutional neural networks (CNNs),
convolutions with kernel size &lt;span class="math inline">\(3\times
3\)&lt;/span>, &lt;span class="math inline">\(5\times 5\)&lt;/span>, &lt;span
class="math inline">\(7\times 7\)&lt;/span> or even &lt;span
class="math inline">\(11\times 11\)&lt;/span> are often used. In the more
recent literature, however, &lt;span class="math inline">\(1\times
1\)&lt;/span> convolutions are becoming prevalent. In this post, I will try
to explain what &lt;span class="math inline">\(1\times 1\)&lt;/span>
convolutions are and discuss why they are used in CNNs.&lt;/p></description></item></channel></rss>