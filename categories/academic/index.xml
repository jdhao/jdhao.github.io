<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Academic on Blowfish</title><link>https://jdhao.github.io/categories/academic/</link><description>Recent content in Academic on Blowfish</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2017 - 2024 ❤️ jdhao</copyright><lastBuildDate>Sat, 16 Oct 2021 09:54:49 +0800</lastBuildDate><atom:link href="https://jdhao.github.io/categories/academic/index.xml" rel="self" type="application/rss+xml"/><item><title>Why Use Cross Entropy in Classification Task?</title><link>https://jdhao.github.io/2021/10/16/why_cross_entropy_in_classification/</link><pubDate>Sat, 16 Oct 2021 09:54:49 +0800</pubDate><guid>https://jdhao.github.io/2021/10/16/why_cross_entropy_in_classification/</guid><description>&lt;p>In classification tasks, the de facto loss to use is the &lt;a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" target="_blank">
cross entropy loss&lt;/a>.&lt;/p></description></item><item><title>A Few Grammar Questions in Writing</title><link>https://jdhao.github.io/2019/09/17/writing_grammar_s1/</link><pubDate>Tue, 17 Sep 2019 00:02:16 +0800</pubDate><guid>https://jdhao.github.io/2019/09/17/writing_grammar_s1/</guid><description>&lt;p>A few grammar-related questions in writing scientific papers.&lt;/p></description></item><item><title>两个大规模中文语料库介绍以及处理</title><link>https://jdhao.github.io/2019/01/10/two_chinese_corpus/</link><pubDate>Thu, 10 Jan 2019 00:27:00 +0800</pubDate><guid>https://jdhao.github.io/2019/01/10/two_chinese_corpus/</guid><description>&lt;p>目前进行的工作需要大规模的语料库来生成中文文本图像，因此查找资料，找了一些中文语料库。本文介绍其中的两个最大的语料库，THUCNews 语料库和中文维基百科语料库，以及如何对原始语料库文件进行简单预处理。&lt;/p></description></item><item><title>使用 Microsoft Word 撰写论文常见问题总结</title><link>https://jdhao.github.io/2018/05/10/write-paper-word-issues/</link><pubDate>Thu, 10 May 2018 21:04:00 +0800</pubDate><guid>https://jdhao.github.io/2018/05/10/write-paper-word-issues/</guid><description>本文总结一些使用 Microsoft Word 2016 撰写论文时遇到的一些问题以及解决办法。
Word 如何插入分节符？ # 「分节符」是 Word 中一个非常重要概念，很多设置都会涉及到分节符。分节符，顾名思义，就是把文档从某处分节（节的英语为 section）的标志。常用的分节符类型有「下一页」（即从当前位置的下一页开始新的一节），「连续」（即从当前位置开始新的一节）。具体如何插入分节符见下图（以插入「下一页」分节符为例）：
Word 中如何显示文中所有的控制符号，例如分页符，分节符等？ # 在 Word 中，默认情况下，分页符，分节符等控制符号，用户是看不到的，如果你不小心多插入了分节符，可能会造成意想不到的结果，但是很难找出原因，因此建议把显示控制符号的选项打开，帮助你了解文档中都有哪些字符。
如何打开这个选项？点击「开始」选项卡，然后找到「段落」部分，打开「显示/隐藏编辑标记」选项，如下图：
或者使用 Word 提供的快捷键，Ctrl+* 来开启或关闭这项功能。
如何在目录部分不显示目录本身的页码 # 你可能会遇到这种情况：在目录部分，目录本身作为第一项显示在上面（如下图所示）。
通常我们希望目录第一项能从摘要开始，不要包含目录自身。之所以出现目录自身，是因为目录也采用了一级标题的样式，因此生成目录的时候，Word 自动把目录本身也加入了进去。我们可以先清除目录的格式，然后再手动把目录设置为一级标题要求的字体以及字号，然后再更新目录，你会发现第一项从摘要开始了。
清除目录格式很简单，首先选中目录，在「开始」选项卡下，找到「样式」，然后点击下拉菜单，找到「清除格式」，点击即可（见下图）。
如何更新目录？点击「引用」选项卡，在目录部分，点击「更新目录」，然后选择「更新整个目录」即可，见下图：
如何在 Word 左边显示当前文档的目录导航？ # 在 Word 文档左边显示当前文档的结构，可以方便跳转到某一部分，作用类似 PDF 文件中的书签功能。如何开启？在 Word 中点击「视图」选项卡，然后找到「显示」相关的选项，选中「导航窗口」，目录就会出现在文档的左边（前提是你已经正确设置了各级标题），如下图所示：
如何从摘要部分开始文档的页码？ # 摘要之前一般为目录，把光标置于目录页最后一页（假设目录有多页），然后插入一个类型为「下一页」分节符，你会看到目录页最后一页多了一个「分节符」的标志：
然后如果此时双击该页的页脚，Word 会提示文档每一页位于哪一节，你会发现目录页与正文的摘要处于不同的节，这正是我们需要的效果，见下图，
然后把光标移到下一节页脚的位置，单击「设计」选项卡，找到「页眉与页脚」部分，点击「页码」下拉菜单，
然后在菜单中选择「设置页码格式」，在弹出的窗口使用下面的设置，
如果目录页以及目录页之前的部分有页码出现，直接把光标移动到目录页页脚，删掉那些页码即可，这样论文就是从摘要开始编码了，起始页码为 1。
如何使得表格的宽度等于页面的宽度 # 在 Word 中，有的表格宽度很窄，如何使得表格的宽度等于当前 Word 页面的宽度呢？首先选中整个表格，然后单击「布局」选项卡，找到「单元格大小」栏目，点击「自动调整」下拉菜单，然后选择「根据窗口自动调整表格」选项，见下图
如何插入公式 # Word 中自带的插入公式功能简直是噩梦，效率极其低下。一种解决的办法就是安装第三方的公式辅助插件，例如 MathType（国外），或者 AxMath（国内），这两款软件都是收费的，但是后者价格更低，并且看起来做的更美观，可以考虑购买。另外一个非常不错的免费插件，叫做 texsword，可以直接在其中书写 LaTeX 公式，生成图片，插入到 Word 中，具体安装使用参见 这里。
如果对 LaTeX 比较熟悉1，还可以安装 Pandoc，</description></item><item><title>使用 Endnote 为论文每个章节制作单独的参考文献列表</title><link>https://jdhao.github.io/2018/03/24/separate-reference-with-endnote/</link><pubDate>Sat, 24 Mar 2018 15:18:00 +0800</pubDate><guid>https://jdhao.github.io/2018/03/24/separate-reference-with-endnote/</guid><description>&lt;p>毕业季到了，如果使用 Word 来写毕业论文，论文写作中的参考文献处理也是一个非常重要的问题，在 Word 管理参考的比较好的方式是通过 &lt;a href="http://www.ioa.cas.cn/xwzx/zhxw/201410/P020141013364219434960.pdf" target="_blank">
Endnote&lt;/a>。Endnote 是一个非常方便的可视化管理参考文献的工具，本文介绍如何使用 Endnote 结合 Microsoft Word 给每个章节分别设置参考文献列表。&lt;/p></description></item><item><title>Similarity Measurement in Image Retrieval</title><link>https://jdhao.github.io/2017/10/24/image-similarity-measure-image-retrieval/</link><pubDate>Tue, 24 Oct 2017 20:38:52 +0800</pubDate><guid>https://jdhao.github.io/2017/10/24/image-similarity-measure-image-retrieval/</guid><description>&lt;p>For image retrieval and other similarity-based tasks such as &lt;a
href="https://arxiv.org/abs/1610.02984">person re-identification&lt;/a>, we
need to compute the similarity (or distance) between the query image and
database images. Then we can rank the database images based on their
similarity to the query image. In this post, I want to briefly introduce
two measures widely used in image retrieval tasks.&lt;/p></description></item><item><title>Some Loss Functions and Their Intuitive Explanations</title><link>https://jdhao.github.io/2017/03/13/some_loss_and_explanations/</link><pubDate>Mon, 13 Mar 2017 10:14:55 +0800</pubDate><guid>https://jdhao.github.io/2017/03/13/some_loss_and_explanations/</guid><description>&lt;p>Loss functions are frequently used in supervised machine learning to
minimize the differences between the predicted output of the model and
the ground truth labels. In other words, it is used to measure how good
our model can predict the true class of a sample from the dataset. Here
I would like to list some frequently-used loss functions and give my
intuitive explanation.&lt;/p></description></item><item><title>关于神经网络优化的一些思考</title><link>https://jdhao.github.io/2016/11/25/thought-on-optimization-for-cnn/</link><pubDate>Fri, 25 Nov 2016 00:00:00 +0000</pubDate><guid>https://jdhao.github.io/2016/11/25/thought-on-optimization-for-cnn/</guid><description>&lt;p>优化的方法有很多种，在深度学习中，占有绝对主导地位的还是 stochastic gradient
optimization (简称 SGD) 以及它的一些变种，如 SGD with momentum，Adam 等。 SGD
是一种基于一阶梯度信息的优化方法，仅从优化的速度上来讲，效率不是最高的，一些利
用二阶信息的优化方法，理论上优化速度更快，但是，SGD 反而是在深度学习的优化中使
用的最多的优化方法，为什么其他类型的优化方法在深度学习中不经常使用呢？它们相比
SGD 有什么缺点？或者说 SGD 有什么优点呢？ 这篇文章试图对这个问题给出自己的思考
与总结。&lt;/p></description></item><item><title>神经网络中误差反向传播(back propagation)算法的工作原理</title><link>https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/</link><pubDate>Tue, 19 Jan 2016 00:00:00 +0800</pubDate><guid>https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/</guid><description>&lt;p>神经网络，从大学时期就听说过，后面上课的时候老师也讲过，但是感觉从来没有真正掌握，总是似是而非，比较模糊，好像懂，其实并不懂。&lt;/p>
&lt;p>为了搞懂神经网络的运行原理，有必要搞清楚神经网络最核心的算法，也就是&lt;a
href="https://en.wikipedia.org/wiki/Backpropagation">误差反向传播算法&lt;/a>的工作原理，本文以最简单的全连接神经网络为例，介绍误差反向传播算法的原理。&lt;/p></description></item></channel></rss>