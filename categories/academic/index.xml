<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Academic on jdhao's digital space</title><link>https://jdhao.github.io/categories/academic/</link><description>Recent content in Academic on jdhao's digital space</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2017 - 2024 ❤️ jdhao</copyright><lastBuildDate>Sat, 16 Oct 2021 09:54:49 +0800</lastBuildDate><atom:link href="https://jdhao.github.io/categories/academic/index.xml" rel="self" type="application/rss+xml"/><item><title>Why Use Cross Entropy in Classification Task?</title><link>https://jdhao.github.io/2021/10/16/why_cross_entropy_in_classification/</link><pubDate>Sat, 16 Oct 2021 09:54:49 +0800</pubDate><guid>https://jdhao.github.io/2021/10/16/why_cross_entropy_in_classification/</guid><description>&lt;p>In classification tasks, the de facto loss to use is the &lt;a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" target="_blank">cross entropy loss&lt;/a>.&lt;/p></description></item><item><title>A Few Grammar Questions in Writing</title><link>https://jdhao.github.io/2019/09/17/writing_grammar_s1/</link><pubDate>Tue, 17 Sep 2019 00:02:16 +0800</pubDate><guid>https://jdhao.github.io/2019/09/17/writing_grammar_s1/</guid><description>&lt;p>A few grammar-related questions in writing scientific papers.&lt;/p></description></item><item><title>两个大规模中文语料库介绍以及处理</title><link>https://jdhao.github.io/2019/01/10/two_chinese_corpus/</link><pubDate>Thu, 10 Jan 2019 00:27:00 +0800</pubDate><guid>https://jdhao.github.io/2019/01/10/two_chinese_corpus/</guid><description>&lt;p>目前进行的工作需要大规模的语料库来生成中文文本图像，因此查找资料，找了一些中文语料库。本文介绍其中的两个最大的语料库，THUCNews 语料库和中文维基百科语料库，以及如何对原始语料库文件进行简单预处理。&lt;/p></description></item><item><title>使用 Microsoft Word 撰写论文常见问题总结</title><link>https://jdhao.github.io/2018/05/10/write-paper-word-issues/</link><pubDate>Thu, 10 May 2018 21:04:00 +0800</pubDate><guid>https://jdhao.github.io/2018/05/10/write-paper-word-issues/</guid><description>本文总结一些使用 Microsoft Word 2016 撰写论文时遇到的一些问题以及解决办法。 Word 如何插入分节符？ # 「分节符」是 Word 中一个非常重要概念，很多设置都会涉及到分节符。</description></item><item><title>使用 Endnote 为论文每个章节制作单独的参考文献列表</title><link>https://jdhao.github.io/2018/03/24/separate-reference-with-endnote/</link><pubDate>Sat, 24 Mar 2018 15:18:00 +0800</pubDate><guid>https://jdhao.github.io/2018/03/24/separate-reference-with-endnote/</guid><description>&lt;p>毕业季到了，如果使用 Word 来写毕业论文，论文写作中的参考文献处理也是一个非常重要的问题，在 Word 管理参考的比较好的方式是通过 &lt;a href="http://www.ioa.cas.cn/xwzx/zhxw/201410/P020141013364219434960.pdf" target="_blank">Endnote&lt;/a>。Endnote 是一个非常方便的可视化管理参考文献的工具，本文介绍如何使用 Endnote 结合 Microsoft Word 给每个章节分别设置参考文献列表。&lt;/p></description></item><item><title>Similarity Measurement in Image Retrieval</title><link>https://jdhao.github.io/2017/10/24/image-similarity-measure-image-retrieval/</link><pubDate>Tue, 24 Oct 2017 20:38:52 +0800</pubDate><guid>https://jdhao.github.io/2017/10/24/image-similarity-measure-image-retrieval/</guid><description>&lt;p>For image retrieval and other similarity-based tasks such as &lt;a
href="https://arxiv.org/abs/1610.02984">person re-identification&lt;/a>, we
need to compute the similarity (or distance) between the query image and
database images. Then we can rank the database images based on their
similarity to the query image. In this post, I want to briefly introduce
two measures widely used in image retrieval tasks.&lt;/p></description></item><item><title>Some Loss Functions and Their Intuitive Explanations</title><link>https://jdhao.github.io/2017/03/13/some_loss_and_explanations/</link><pubDate>Mon, 13 Mar 2017 10:14:55 +0800</pubDate><guid>https://jdhao.github.io/2017/03/13/some_loss_and_explanations/</guid><description>&lt;p>Loss functions are frequently used in supervised machine learning to
minimize the differences between the predicted output of the model and
the ground truth labels. In other words, it is used to measure how good
our model can predict the true class of a sample from the dataset. Here
I would like to list some frequently-used loss functions and give my
intuitive explanation.&lt;/p></description></item><item><title>关于神经网络优化的一些思考</title><link>https://jdhao.github.io/2016/11/25/thought-on-optimization-for-cnn/</link><pubDate>Fri, 25 Nov 2016 00:00:00 +0000</pubDate><guid>https://jdhao.github.io/2016/11/25/thought-on-optimization-for-cnn/</guid><description>&lt;p>优化的方法有很多种，在深度学习中，占有绝对主导地位的还是 stochastic gradient
optimization (简称 SGD) 以及它的一些变种，如 SGD with momentum，Adam 等。 SGD
是一种基于一阶梯度信息的优化方法，仅从优化的速度上来讲，效率不是最高的，一些利
用二阶信息的优化方法，理论上优化速度更快，但是，SGD 反而是在深度学习的优化中使
用的最多的优化方法，为什么其他类型的优化方法在深度学习中不经常使用呢？它们相比
SGD 有什么缺点？或者说 SGD 有什么优点呢？ 这篇文章试图对这个问题给出自己的思考
与总结。&lt;/p></description></item><item><title>神经网络中误差反向传播(back propagation)算法的工作原理</title><link>https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/</link><pubDate>Tue, 19 Jan 2016 00:00:00 +0800</pubDate><guid>https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/</guid><description>&lt;p>神经网络，从大学时期就听说过，后面上课的时候老师也讲过，但是感觉从来没有真正掌握，总是似是而非，比较模糊，好像懂，其实并不懂。&lt;/p>
&lt;p>为了搞懂神经网络的运行原理，有必要搞清楚神经网络最核心的算法，也就是&lt;a
href="https://en.wikipedia.org/wiki/Backpropagation">误差反向传播算法&lt;/a>的工作原理，本文以最简单的全连接神经网络为例，介绍误差反向传播算法的原理。&lt;/p></description></item></channel></rss>