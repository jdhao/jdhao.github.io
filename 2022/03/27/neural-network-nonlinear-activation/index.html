<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Nonlinear Activations for Neural Networks &#183; Blowfish</title>
<meta name=title content="Nonlinear Activations for Neural Networks &#183; Blowfish"><link rel=canonical href=https://jdhao.github.io/2022/03/27/neural-network-nonlinear-activation/><link type=text/css rel=stylesheet href=/css/main.bundle.min.85f006de1806b55b2d082d1122deedd8cec6fdc1f7c8a51be975a4496165d6c798f4a3de52545a6a87cc4bc09fc2d6f5201d44ea57acc1ac2b1c42a688c62bee.css integrity="sha512-hfAG3hgGtVstCC0RIt7t2M7G/cH3yKUb6XWkSWFl1seY9KPeUlRaaofMS8Cfwtb1IB1E6leswawrHEKmiMYr7g=="><script type=text/javascript src=/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.55d70af2ac9cbc52f4b3a22c1562a4924d00f0ec10e2758a0a502414b89b74ae7a50fd604ffb88d2fb82e43831a23ed56326a26580117701f20483b61efd902e.js integrity="sha512-VdcK8qycvFL0s6IsFWKkkk0A8OwQ4nWKClAkFLibdK56UP1gT/uI0vuC5Dgxoj7VYyaiZYARdwHyBIO2Hv2QLg==" data-copy data-copied></script><script src=/js/zoom.min.js></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta name=google-site-verification content="HTz0VHxqny_b0FfS774dICLBzHGBZCb_S11j_akF1Tw"><meta property="og:title" content="Nonlinear Activations for Neural Networks"><meta property="og:description" content="


Non-linear activations are important in deep neural networks. It is
important in the sense that without non-linear activation functions,
even if you have many linear layers, the end results is like you have
only one linear layer, and the approximation ability of the network is
very limited1. Some of most commonly-used
nonlinear activation functions are Sigmoid, ReLU and Tanh."><meta property="og:type" content="article"><meta property="og:url" content="https://jdhao.github.io/2022/03/27/neural-network-nonlinear-activation/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-03-27T17:25:12+08:00"><meta property="article:modified_time" content="2022-03-27T17:50:40+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Nonlinear Activations for Neural Networks"><meta name=twitter:description content="


Non-linear activations are important in deep neural networks. It is
important in the sense that without non-linear activation functions,
even if you have many linear layers, the end results is like you have
only one linear layer, and the approximation ability of the network is
very limited1. Some of most commonly-used
nonlinear activation functions are Sigmoid, ReLU and Tanh."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"Nonlinear Activations for Neural Networks","headline":"Nonlinear Activations for Neural Networks","abstract":"\u003cp align=\u0022center\u0022\u003e\n\u003cimg src=\u0022https:\/\/blog-resource-1257868508.file.myqcloud.com\/202203271740705.png\u0022 width=\u0022600\u0022\u003e\n\u003c\/p\u003e\n\u003cp\u003eNon-linear activations are important in deep neural networks. It is\nimportant in the sense that without non-linear activation functions,\neven if you have many linear layers, the end results is like you have\nonly one linear layer, and the approximation ability of the network is\nvery limited\u003ca href=\u0022#fn1\u0022 class=\u0022footnote-ref\u0022 id=\u0022fnref1\u0022\nrole=\u0022doc-noteref\u0022\u003e\u003csup\u003e1\u003c\/sup\u003e\u003c\/a\u003e. Some of most commonly-used\nnonlinear activation functions are Sigmoid, ReLU and Tanh.\u003c\/p\u003e","inLanguage":"en","url":"https:\/\/jdhao.github.io\/2022\/03\/27\/neural-network-nonlinear-activation\/","author":{"@type":"Person","name":"jdhao"},"copyrightYear":"2022","dateCreated":"2022-03-27T17:25:12\u002b08:00","datePublished":"2022-03-27T17:25:12\u002b08:00","dateModified":"2022-03-27T17:50:40\u002b02:00","mainEntityOfPage":"true","wordCount":"586"}]</script><meta name=author content="jdhao"><script src=/lib/jquery/jquery.slim.min.js integrity></script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium text-gray-500 hover:text-gray-900">Blowfish</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Home</p></a><a href=/archives/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archives</p></a><a href=/categories/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Categories</p></a><a href=/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=About>About</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button for=menu-controller class=block><input type=checkbox id=menu-controller class=hidden><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Home</p></a></li><li class=mt-1><a href=/archives/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archives</p></a></li><li class=mt-1><a href=/categories/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Categories</p></a></li><li class=mt-1><a href=/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=About>About</p></a></li></ul></div></label></div></div><div class="relative flex flex-col grow"><main id=main-content class=grow><article><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/>Blowfish</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/archives/>Posts</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/2022/03/27/neural-network-nonlinear-activation/>Nonlinear Activations for Neural Networks</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Nonlinear Activations for Neural Networks</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2022-03-27 17:25:12 +0800 +0800">27 March 2022</time><span class="px-2 text-primary-500">&#183;</span><span>586 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">3 mins</span><span class="px-2 text-primary-500">&#183;</span>
<script type=text/javascript src=/js/zen-mode.min.63c8a202661f4a2063fdc2706685d668e8ea3da613da2224e9da527e5876e4f53dcac39ab60732626fb4151feae5d430d0cf44731e5d3c726522fcc1519c1547.js integrity="sha512-Y8iiAmYfSiBj/cJwZoXWaOjqPaYT2iIk6dpSflh25PU9ysOatgcyYm+0FR/q5dQw0M9Ecx5dPHJlIvzBUZwVRw=="></script><span class=mb-[2px]><span id=zen-mode-button class="text-lg hover:text-primary-500" title="Enable zen mode" data-title-i18n-disable="Enable zen mode" data-title-i18n-enable="Disable zen mode"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="50" height="50"><path fill="currentcolor" d="M12.980469 4C9.1204688 4 5.9804688 7.14 5.9804688 11L6 26H9.9804688V11c0-1.65 1.3400002-3 3.0000002-3H40.019531c1.66.0 3 1.35 3 3V39c0 1.65-1.34 3-3 3H29c0 1.54-.579062 2.94-1.539062 4H40.019531c3.86.0 7-3.14 7-7V11c0-3.86-3.14-7-7-7H12.980469zM7 28c-2.206.0-4 1.794-4 4V42c0 2.206 1.794 4 4 4H23c2.206.0 4-1.794 4-4V32c0-2.206-1.794-4-4-4H7zm0 4H23L23.001953 42H7V32z"/></svg></span></span></span></span></div></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><p align=center><img src=https://blog-resource-1257868508.file.myqcloud.com/202203271740705.png width=600></p><p>Non-linear activations are important in deep neural networks. It is
important in the sense that without non-linear activation functions,
even if you have many linear layers, the end results is like you have
only one linear layer, and the approximation ability of the network is
very limited<a href=#fn1 class=footnote-ref id=fnref1 role=doc-noteref><sup>1</sup></a>. Some of most commonly-used
nonlinear activation functions are Sigmoid, ReLU and Tanh.</p><h1 id=nonlinear-activations-and-their-derivatives>Nonlinear
activations and their derivatives</h1><h2 id=sigmoid>Sigmoid</h2><p>Sigmoid function, also known as logistic function, has the following
form:</p><p><span class="math display">\[f(x) = \frac{1}{1+e^{-x}}\]</span></p><p>The derivative of sigmoid is:</p><p><span class="math display">\[\begin{aligned}\frac{df}{dx} &=
\frac{e^{-x}}{(1+e^{-x})^2}\\
&= \frac{1}{1+e^{-x}}(1-
\frac{1}{1+e^{-x}})\\
&= f(x)(1-f(x))
\end{aligned}\]</span></p><h2 id=tanh>Tanh</h2><p><a href=https://en.wikipedia.org/wiki/Hyperbolic_functions#Exponential_definitions>Tanh</a>
function</p><p><span class="math display">\[f(x) =
\frac{e^{2x}-1}{e^{2x}+1}\]</span></p><p>The derivative of Tanh is:</p><p><span class="math display">\[\frac{df}{dx} = \frac{4e^{2x}}{(e^{2x} +
1)^2} = 1 - {f(x)}^2\]</span></p><h2 id=relu>ReLU</h2><p>ReLU, called rectified linear unit, has the following form:</p><p><span class="math display">\[f(x) = \max(0, x)\]</span></p><p>We can also write ReLU as:</p><p><span class="math display">\[f(x) =
\begin{cases}
x & x \geq 0 \\
0 & x &lt; 0
\end{cases}\]</span></p><p>The derivate of ReLU is quite simple, it is <code>1</code> for <span class="math inline">\(x > 0\)</span> and 0 otherwise.</p><p>There are also variants of ReLU, such as Leaky ReLU, PReLU
(parametric ReLU), and RReLU (randomized ReLU). In <a href=https://arxiv.org/abs/1505.00853>Empirical Evaluation of
Rectified Activations in Convolutional Network</a>, the author claimed
that PReLU and RReLU works better than ReLU in small scale datasets such
as CIFAR10, CIFAR100 and <a href=https://www.kaggle.com/c/datasciencebowl>Kaggle NDSB</a>.</p><h1 id=vanishing-gradient>Vanishing gradient</h1><p>I show the plot of different activation functions and their
derivatives in the title image.</p><details><summary><font size=2 color=red>Click to show the code for
visualization.</font></summary><div class=sourceCode id=cb1><pre class="sourceCode python"><code class="sourceCode python"><span id=cb1-1><a href=#cb1-1 aria-hidden=true tabindex=-1></a><span class=im>import</span> matplotlib.pyplot <span class=im>as</span> plt</span>
<span id=cb1-2><a href=#cb1-2 aria-hidden=true tabindex=-1></a><span class=im>import</span> numpy <span class=im>as</span> np</span>
<span id=cb1-3><a href=#cb1-3 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-4><a href=#cb1-4 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-5><a href=#cb1-5 aria-hidden=true tabindex=-1></a><span class=kw>def</span> main():</span>
<span id=cb1-6><a href=#cb1-6 aria-hidden=true tabindex=-1></a>    x <span class=op>=</span> np.linspace(<span class=op>-</span><span class=dv>5</span>, <span class=dv>5</span>, <span class=dv>100</span>)</span>
<span id=cb1-7><a href=#cb1-7 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-8><a href=#cb1-8 aria-hidden=true tabindex=-1></a>    r <span class=op>=</span> [relu(v) <span class=cf>for</span> v <span class=kw>in</span> x]</span>
<span id=cb1-9><a href=#cb1-9 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-10><a href=#cb1-10 aria-hidden=true tabindex=-1></a>    sig <span class=op>=</span> [sigmoid(v) <span class=cf>for</span> v <span class=kw>in</span> x]</span>
<span id=cb1-11><a href=#cb1-11 aria-hidden=true tabindex=-1></a>    d_sig <span class=op>=</span> [sigmoid(v)<span class=op>*</span>(<span class=dv>1</span> <span class=op>-</span> sigmoid(v)) <span class=cf>for</span> v <span class=kw>in</span> x]</span>
<span id=cb1-12><a href=#cb1-12 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-13><a href=#cb1-13 aria-hidden=true tabindex=-1></a>    t <span class=op>=</span> [tanh(v) <span class=cf>for</span> v <span class=kw>in</span> x]</span>
<span id=cb1-14><a href=#cb1-14 aria-hidden=true tabindex=-1></a>    d_tanh <span class=op>=</span> [<span class=dv>1</span> <span class=op>-</span> tanh(v)<span class=op>**</span><span class=dv>2</span> <span class=cf>for</span> v <span class=kw>in</span> x]</span>
<span id=cb1-15><a href=#cb1-15 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-16><a href=#cb1-16 aria-hidden=true tabindex=-1></a>    fig <span class=op>=</span> plt.figure(figsize<span class=op>=</span>[<span class=dv>6</span>, <span class=dv>3</span>])</span>
<span id=cb1-17><a href=#cb1-17 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-18><a href=#cb1-18 aria-hidden=true tabindex=-1></a>    ax <span class=op>=</span> fig.add_subplot(<span class=dv>111</span>)</span>
<span id=cb1-19><a href=#cb1-19 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-20><a href=#cb1-20 aria-hidden=true tabindex=-1></a>    ax.plot(x, r, <span class=st>&#39;#66c2a5&#39;</span>, label<span class=op>=</span><span class=st>&#39;ReLU&#39;</span>)</span>
<span id=cb1-21><a href=#cb1-21 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-22><a href=#cb1-22 aria-hidden=true tabindex=-1></a>    ax.plot(x, sig, <span class=st>&#39;#fc8d62&#39;</span>, label<span class=op>=</span><span class=st>&#39;sigmoid&#39;</span>)</span>
<span id=cb1-23><a href=#cb1-23 aria-hidden=true tabindex=-1></a>    ax.plot(x, d_sig, <span class=st>&#39;#8da0cb&#39;</span>, label<span class=op>=</span><span class=st>&#39;sigmoid derivative&#39;</span>)</span>
<span id=cb1-24><a href=#cb1-24 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-25><a href=#cb1-25 aria-hidden=true tabindex=-1></a>    ax.plot(x, t, <span class=st>&#39;#e78ac3&#39;</span>, label<span class=op>=</span><span class=st>&#39;tanh&#39;</span>)</span>
<span id=cb1-26><a href=#cb1-26 aria-hidden=true tabindex=-1></a>    ax.plot(x, d_tanh, <span class=st>&#39;#a6d854&#39;</span>, label<span class=op>=</span><span class=st>&#39;tanh derivative&#39;</span>)</span>
<span id=cb1-27><a href=#cb1-27 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-28><a href=#cb1-28 aria-hidden=true tabindex=-1></a>    ax.legend()</span>
<span id=cb1-29><a href=#cb1-29 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-30><a href=#cb1-30 aria-hidden=true tabindex=-1></a>    plt.savefig(<span class=st>&#39;activation-curve.png&#39;</span>, dpi<span class=op>=</span><span class=dv>96</span>, bbox_inches<span class=op>=</span><span class=st>&#39;tight&#39;</span>)</span>
<span id=cb1-31><a href=#cb1-31 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-32><a href=#cb1-32 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-33><a href=#cb1-33 aria-hidden=true tabindex=-1></a><span class=kw>def</span> relu(x):</span>
<span id=cb1-34><a href=#cb1-34 aria-hidden=true tabindex=-1></a>    <span class=cf>if</span> x <span class=op>&gt;=</span><span class=dv>0</span>:</span>
<span id=cb1-35><a href=#cb1-35 aria-hidden=true tabindex=-1></a>        <span class=cf>return</span> x</span>
<span id=cb1-36><a href=#cb1-36 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-37><a href=#cb1-37 aria-hidden=true tabindex=-1></a>    <span class=cf>return</span> <span class=dv>0</span></span>
<span id=cb1-38><a href=#cb1-38 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-39><a href=#cb1-39 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-40><a href=#cb1-40 aria-hidden=true tabindex=-1></a><span class=kw>def</span> sigmoid(x):</span>
<span id=cb1-41><a href=#cb1-41 aria-hidden=true tabindex=-1></a>    <span class=cf>return</span> <span class=dv>1</span><span class=op>/</span>(<span class=dv>1</span> <span class=op>+</span> np.exp(<span class=op>-</span>x))</span>
<span id=cb1-42><a href=#cb1-42 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-43><a href=#cb1-43 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-44><a href=#cb1-44 aria-hidden=true tabindex=-1></a><span class=kw>def</span> tanh(x):</span>
<span id=cb1-45><a href=#cb1-45 aria-hidden=true tabindex=-1></a>    <span class=cf>return</span> (np.exp(x)<span class=op>**</span><span class=dv>2</span> <span class=op>-</span> <span class=dv>1</span>)<span class=op>/</span>(np.exp(x)<span class=op>**</span><span class=dv>2</span> <span class=op>+</span> <span class=dv>1</span>)</span>
<span id=cb1-46><a href=#cb1-46 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-47><a href=#cb1-47 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-48><a href=#cb1-48 aria-hidden=true tabindex=-1></a><span class=cf>if</span> <span class=va>__name__</span> <span class=op>==</span> <span class=st>&quot;__main__&quot;</span>:</span>
<span id=cb1-49><a href=#cb1-49 aria-hidden=true tabindex=-1></a>    main()</span></code></pre></div></details><p>The derivative of sigmoid is relatively small, and its largest value
is only 0.25 (when <span class="math inline">\(x = 0\)</span>). When
<span class="math inline">\(x\)</span> is large, the derivative is near
zero. Tanh has a similar issue: it has a low gradient, and maximum
gradient is only 1 (<span class="math inline">\(x=0\)</span>).</p><p>This will cause the vanishing gradient problem, because in order to
calculate the derivative of loss w.r.t the weight of earlier layers in
the network, we need to multiply the gradient in the later layers. When
you multiply several values below 0.25, the result goes down to zero
quickly, so the network weight in earlier layers get updated slowly. In
other words, the learning process will converge much slower than using
ReLU, and we might need much more epochs to get a satisfactory
result.</p><p>Another advantage of ReLU is that it is computationally cheap
compared to sigmoid, both in terms of forward and backward
operation.</p><h1 id=try-it-yourself-interactively>Try it yourself
interactively</h1><p>To gain more insight into this, we can use <a href=https://cs.stanford.edu/~karpathy/convnetjs/demo/mnist.html>minist
on convenet.js</a> and change the activation function to see how the
train goes. We can see that training process under tanh and sigmoid
activation is much slower than ReLU. Sigmoid is slowest among the
three.</p><p>We can also play with different activations functions real quick with
<a href=https://playground.tensorflow.org/>TensorFlow
playground</a>.</p><h1 id=references>References</h1><ul><li><a href=https://en.wikipedia.org/wiki/Rectifier_(neural_networks) class=uri>https://en.wikipedia.org/wiki/Rectifier_(neural_networks)</a></li><li>A list of activation functions: <a href=https://en.wikipedia.org/wiki/Activation_function class=uri>https://en.wikipedia.org/wiki/Activation_function</a></li><li>ReLU vs Sigmoid: <a href=https://stats.stackexchange.com/q/126238/140049 class=uri>https://stats.stackexchange.com/q/126238/140049</a></li></ul><section id=footnotes class="footnotes footnotes-end-of-document" role=doc-endnotes><hr><ol><li id=fn1><p>See <a href=https://stats.stackexchange.com/a/335972/140049>this post</a> and
also <a href=https://stackoverflow.com/q/9782071/6064933>this one</a>
for more detailed discussions.<a href=#fnref1 class=footnote-back role=doc-backlink>↩︎</a></p></li></ol></section></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://jdhao.github.io/2022/03/27/neural-network-nonlinear-activation/&amp;title=Nonlinear%20Activations%20for%20Neural%20Networks" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://jdhao.github.io/2022/03/27/neural-network-nonlinear-activation/&amp;text=Nonlinear%20Activations%20for%20Neural%20Networks" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://reddit.com/submit/?url=https://jdhao.github.io/2022/03/27/neural-network-nonlinear-activation/&amp;resubmit=true&amp;title=Nonlinear%20Activations%20for%20Neural%20Networks" title="Submit to Reddit" aria-label="Submit to Reddit"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M201.5 305.5c-13.8.0-24.9-11.1-24.9-24.6.0-13.8 11.1-24.9 24.9-24.9 13.6.0 24.6 11.1 24.6 24.9.0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4.0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7.0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9.0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5.0 52.6 59.2 95.2 132 95.2 73.1.0 132.3-42.6 132.3-95.2.0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6.0-2.2-2.2-6.1-2.2-8.3.0-2.5 2.5-2.5 6.4.0 8.6 22.8 22.8 87.3 22.8 110.2.0 2.5-2.2 2.5-6.1.0-8.6-2.2-2.2-6.1-2.2-8.3.0zm7.7-75c-13.6.0-24.6 11.1-24.6 24.9.0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.1 24.9-24.6.0-13.8-11-24.9-24.9-24.9z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://pinterest.com/pin/create/bookmarklet/?url=https://jdhao.github.io/2022/03/27/neural-network-nonlinear-activation/&amp;description=Nonlinear%20Activations%20for%20Neural%20Networks" title="Pin on Pinterest" aria-label="Pin on Pinterest"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M496 256c0 137-111 248-248 248-25.6.0-50.2-3.9-73.4-11.1 10.1-16.5 25.2-43.5 30.8-65 3-11.6 15.4-59 15.4-59 8.1 15.4 31.7 28.5 56.8 28.5 74.8.0 128.7-68.8 128.7-154.3.0-81.9-66.9-143.2-152.9-143.2-107 0-163.9 71.8-163.9 150.1.0 36.4 19.4 81.7 50.3 96.1 4.7 2.2 7.2 1.2 8.3-3.3.8-3.4 5-20.3 6.9-28.1.6-2.5.3-4.7-1.7-7.1-10.1-12.5-18.3-35.3-18.3-56.6.0-54.7 41.4-107.6 112-107.6 60.9.0 103.6 41.5 103.6 100.9.0 67.1-33.9 113.6-78 113.6-24.3.0-42.6-20.1-36.7-44.8 7-29.5 20.5-61.3 20.5-82.6.0-19-10.2-34.9-31.4-34.9-24.9.0-44.9 25.7-44.9 60.2.0 22 7.4 36.8 7.4 36.8s-24.5 103.8-29 123.2c-5 21.4-3 51.6-.9 71.2C65.4 450.9.0 361.1.0 256 0 119 111 8 248 8s248 111 248 248z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.facebook.com/sharer/sharer.php?u=https://jdhao.github.io/2022/03/27/neural-network-nonlinear-activation/&amp;quote=Nonlinear%20Activations%20for%20Neural%20Networks" title="Share on Facebook" aria-label="Share on Facebook"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14.0 55.52 4.84 55.52 4.84v61h-31.28c-30.8.0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://jdhao.github.io/2022/03/27/neural-network-nonlinear-activation/&amp;subject=Nonlinear%20Activations%20for%20Neural%20Networks" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://api.whatsapp.com/send?text=https://jdhao.github.io/2022/03/27/neural-network-nonlinear-activation/&amp;resubmit=true&amp;title=Nonlinear%20Activations%20for%20Neural%20Networks" title aria-label><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4.0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3.0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2.0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2.0-101.7 82.8-184.5 184.6-184.5 49.3.0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5.0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8s-14.3 18-17.6 21.8c-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2s-9.7 1.4-14.8 6.9c-5.1 5.6-19.4 19-19.4 46.3.0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://t.me/share/url?url=https://jdhao.github.io/2022/03/27/neural-network-nonlinear-activation/&amp;resubmit=true&amp;title=Nonlinear%20Activations%20for%20Neural%20Networks" title aria-label><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M248 8C111.033 8 0 119.033.0 256S111.033 504 248 504 496 392.967 496 256 384.967 8 248 8zM362.952 176.66c-3.732 39.215-19.881 134.378-28.1 178.3-3.476 18.584-10.322 24.816-16.948 25.425-14.4 1.326-25.338-9.517-39.287-18.661-21.827-14.308-34.158-23.215-55.346-37.177-24.485-16.135-8.612-25 5.342-39.5 3.652-3.793 67.107-61.51 68.335-66.746.153-.655.3-3.1-1.154-4.384s-3.59-.849-5.135-.5q-3.283.746-104.608 69.142-14.845 10.194-26.894 9.934c-8.855-.191-25.888-5.006-38.551-9.123-15.531-5.048-27.875-7.717-26.8-16.291q.84-6.7 18.45-13.7 108.446-47.248 144.628-62.3c68.872-28.647 83.183-33.623 92.511-33.789 2.052-.034 6.639.474 9.61 2.885a10.452 10.452.0 013.53 6.716A43.765 43.765.0 01362.952 176.66z"/></svg></span></a></section><h2 class="mt-8 text-2xl font-extrabold mb-10">Related</h2><section class="w-full grid gap-4 sm:grid-cols-2 md:grid-cols-3"><a href=/2022/03/18/torch_accelerate_batch_inference/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/2022/03/18/torch_accelerate_batch_inference/>Accelerate Batched Image Inference in PyTorch</div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime="2022-03-18 22:59:33 +0800 +0800">18 March 2022</time><span class="px-2 text-primary-500">&#183;</span><time datetime="2022-03-26 09:40:37.153941388 +0100 CET">Updated: 26 March 2022</time><span class="px-2 text-primary-500">&#183;</span><span>517 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">3 mins</span></div></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/2022/02/27/temperature_in_softmax/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/2022/02/27/temperature_in_softmax/>Softmax with Temperature Explained</div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime="2022-02-27 12:47:02 +0800 +0800">27 February 2022</time><span class="px-2 text-primary-500">&#183;</span><time datetime="2022-02-28 13:53:52.031711692 +0100 CET">Updated: 28 February 2022</time><span class="px-2 text-primary-500">&#183;</span><span>633 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">3 mins</span></div></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/2022/02/09/dependency-hell-build-torch-gpu-docker-container/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/2022/02/09/dependency-hell-build-torch-gpu-docker-container/>Dependency Hell When Building A PyTorch GPU Docker Image</div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime="2022-02-09 21:30:00 +0800 +0800">9 February 2022</time><span class="px-2 text-primary-500">&#183;</span><span>262 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">2 mins</span></div></div></div><div class="px-6 pt-4 pb-2"></div></div></a></section></div><script>var oid="views_posts/neural-network-nonlinear-activation.md",oid_likes="likes_posts/neural-network-nonlinear-activation.md"</script><script type=text/javascript src=/js/page.min.b06a29d42a4ed16787978e2eee1e8c797b7698db2bc14ccee78f5c80ac566fc996190a73ad80a5e987558474b20b96fa38f7d85b405f165ff72b7b163c5ad11b.js integrity="sha512-sGop1CpO0WeHl44u7h6MeXt2mNsrwUzO549cgKxWb8mWGQpzrYCl6YdVhHSyC5b6OPfYW0BfFl/3K3sWPFrRGw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/2022/03/28/how_to_make_zhajiang2/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">地道美味炸酱制作方法</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2022-03-28 00:02:05 +0800 +0800">28 March 2022</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/2022/03/18/torch_accelerate_batch_inference/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Accelerate Batched Image Inference in PyTorch</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2022-03-18 22:59:33 +0800 +0800">18 March 2022</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">© 2017 - 2024 ❤️ jdhao</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.62060bb247f4de2b6dde45903668fefb68d792f365587605177b1227c0cf43588701edaca0cb40e2c8e2789bd5ce67c1d2a215b9fb258c3496a7cd25e7cb5fdf.js integrity="sha512-YgYLskf03itt3kWQNmj++2jXkvNlWHYFF3sSJ8DPQ1iHAe2soMtA4sjieJvVzmfB0qIVufsljDSWp80l58tf3w=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://jdhao.github.io/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>