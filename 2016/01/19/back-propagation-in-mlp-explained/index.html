<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>神经网络中误差反向传播(back propagation)算法的工作原理 &#183; Blowfish</title>
<meta name=title content="神经网络中误差反向传播(back propagation)算法的工作原理 &#183; Blowfish"><meta name=keywords content="optimization,CNN,math,"><link rel=canonical href=https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/><link type=text/css rel=stylesheet href=/css/main.bundle.min.85f006de1806b55b2d082d1122deedd8cec6fdc1f7c8a51be975a4496165d6c798f4a3de52545a6a87cc4bc09fc2d6f5201d44ea57acc1ac2b1c42a688c62bee.css integrity="sha512-hfAG3hgGtVstCC0RIt7t2M7G/cH3yKUb6XWkSWFl1seY9KPeUlRaaofMS8Cfwtb1IB1E6leswawrHEKmiMYr7g=="><script type=text/javascript src=/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.55d70af2ac9cbc52f4b3a22c1562a4924d00f0ec10e2758a0a502414b89b74ae7a50fd604ffb88d2fb82e43831a23ed56326a26580117701f20483b61efd902e.js integrity="sha512-VdcK8qycvFL0s6IsFWKkkk0A8OwQ4nWKClAkFLibdK56UP1gT/uI0vuC5Dgxoj7VYyaiZYARdwHyBIO2Hv2QLg==" data-copy data-copied></script><script src=/js/zoom.min.js></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta name=google-site-verification content="HTz0VHxqny_b0FfS774dICLBzHGBZCb_S11j_akF1Tw"><meta property="og:title" content="神经网络中误差反向传播(back propagation)算法的工作原理"><meta property="og:description" content="神经网络，从大学时期就听说过，后面上课的时候老师也讲过，但是感觉从来没有真正掌握，总是似是而非，比较模糊，好像懂，其实并不懂。
为了搞懂神经网络的运行原理，有必要搞清楚神经网络最核心的算法，也就是误差反向传播算法的工作原理，本文以最简单的全连接神经网络为例，介绍误差反向传播算法的原理。"><meta property="og:type" content="article"><meta property="og:url" content="https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2016-01-19T00:00:00+08:00"><meta property="article:modified_time" content="2022-02-27T05:59:30+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="神经网络中误差反向传播(back propagation)算法的工作原理"><meta name=twitter:description content="神经网络，从大学时期就听说过，后面上课的时候老师也讲过，但是感觉从来没有真正掌握，总是似是而非，比较模糊，好像懂，其实并不懂。
为了搞懂神经网络的运行原理，有必要搞清楚神经网络最核心的算法，也就是误差反向传播算法的工作原理，本文以最简单的全连接神经网络为例，介绍误差反向传播算法的原理。"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"神经网络中误差反向传播(back propagation)算法的工作原理","headline":"神经网络中误差反向传播(back propagation)算法的工作原理","abstract":"\u003cp\u003e神经网络，从大学时期就听说过，后面上课的时候老师也讲过，但是感觉从来没有真正掌握，总是似是而非，比较模糊，好像懂，其实并不懂。\u003c\/p\u003e\n\u003cp\u003e为了搞懂神经网络的运行原理，有必要搞清楚神经网络最核心的算法，也就是\u003ca\nhref=\u0022https:\/\/en.wikipedia.org\/wiki\/Backpropagation\u0022\u003e误差反向传播算法\u003c\/a\u003e的工作原理，本文以最简单的全连接神经网络为例，介绍误差反向传播算法的原理。\u003c\/p\u003e","inLanguage":"en","url":"https:\/\/jdhao.github.io\/2016\/01\/19\/back-propagation-in-mlp-explained\/","author":{"@type":"Person","name":"jdhao"},"copyrightYear":"2016","dateCreated":"2016-01-19T00:00:00\u002b08:00","datePublished":"2016-01-19T00:00:00\u002b08:00","dateModified":"2022-02-27T05:59:30\u002b01:00","keywords":["optimization","CNN","math"],"mainEntityOfPage":"true","wordCount":"527"}]</script><meta name=author content="jdhao"><script src=/lib/jquery/jquery.slim.min.js integrity></script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium text-gray-500 hover:text-gray-900">Blowfish</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Home</p></a><a href=/archives/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archives</p></a><a href=/categories/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Categories</p></a><a href=/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=About>About</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button for=menu-controller class=block><input type=checkbox id=menu-controller class=hidden><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Home</p></a></li><li class=mt-1><a href=/archives/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archives</p></a></li><li class=mt-1><a href=/categories/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Categories</p></a></li><li class=mt-1><a href=/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=About>About</p></a></li></ul></div></label></div></div><div class="relative flex flex-col grow"><main id=main-content class=grow><article><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/>Blowfish</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/archives/>Posts</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/2016/01/19/back-propagation-in-mlp-explained/>神经网络中误差反向传播(back propagation)算法的工作原理</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">神经网络中误差反向传播(back propagation)算法的工作原理</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2016-01-19 00:00:00 +0800 +0800">19 January 2016</time><span class="px-2 text-primary-500">&#183;</span><time datetime="2022-02-27 05:59:30.200515634 +0100 CET">Updated: 27 February 2022</time><span class="px-2 text-primary-500">&#183;</span><span>527 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">3 mins</span><span class="px-2 text-primary-500">&#183;</span>
<script type=text/javascript src=/js/zen-mode.min.63c8a202661f4a2063fdc2706685d668e8ea3da613da2224e9da527e5876e4f53dcac39ab60732626fb4151feae5d430d0cf44731e5d3c726522fcc1519c1547.js integrity="sha512-Y8iiAmYfSiBj/cJwZoXWaOjqPaYT2iIk6dpSflh25PU9ysOatgcyYm+0FR/q5dQw0M9Ecx5dPHJlIvzBUZwVRw=="></script><span class=mb-[2px]><span id=zen-mode-button class="text-lg hover:text-primary-500" title="Enable zen mode" data-title-i18n-disable="Enable zen mode" data-title-i18n-enable="Disable zen mode"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="50" height="50"><path fill="currentcolor" d="M12.980469 4C9.1204688 4 5.9804688 7.14 5.9804688 11L6 26H9.9804688V11c0-1.65 1.3400002-3 3.0000002-3H40.019531c1.66.0 3 1.35 3 3V39c0 1.65-1.34 3-3 3H29c0 1.54-.579062 2.94-1.539062 4H40.019531c3.86.0 7-3.14 7-7V11c0-3.86-3.14-7-7-7H12.980469zM7 28c-2.206.0-4 1.794-4 4V42c0 2.206 1.794 4 4 4H23c2.206.0 4-1.794 4-4V32c0-2.206-1.794-4-4-4H7zm0 4H23L23.001953 42H7V32z"/></svg></span></span></span></span></div></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><p>神经网络，从大学时期就听说过，后面上课的时候老师也讲过，但是感觉从来没有真正掌握，总是似是而非，比较模糊，好像懂，其实并不懂。</p><p>为了搞懂神经网络的运行原理，有必要搞清楚神经网络最核心的算法，也就是<a href=https://en.wikipedia.org/wiki/Backpropagation>误差反向传播算法</a>的工作原理，本文以最简单的全连接神经网络为例，介绍误差反向传播算法的原理。</p><p align=center><img src=https://blog-resource-1257868508.file.myqcloud.com/18-1-24/16883425.jpg title=简单的神经网络示意图></p><p>在开始推导之前，需要先做一些准备工作，推导中所使用的神经网络如上图所示。一个神经网络由多个层
(layer) 构成，每一层有若干个节点
(node)，最左边是「输入层」，中间的层被称为「隐含层」，最右边是「输出层」；上一层节点与下一层节点之间，都有边相连，代表上一层某个节点为下一层某个节点贡献的权值。通常为了使得网络能够模拟复杂的关系，上一层节点的输出乘以权值矩阵得到下一层节点后，需要再经过「激活函数」对得到的各个节点的值进行非线性化处理。激活函数可以选用很多的形式，这里使用sigmoid
函数，表达式如下：</p><p><span class="math display">\[\begin{equation}\label{eq1}
f(x)=\frac{1}{1+\exp(-x)}
\end{equation}\]</span></p><p>公式 <span class="math inline">\(\eqref{eq1}\)</span> 之所以使用
sigmoid 函数，一个很重要的原因就是「mathematical convenience」，sigmoid
函数的导数很好计算，</p><p><span class="math display">\[\begin{equation}
f'(x)=f(x)(1-f(x))
\end{equation}\]</span></p><p>接下来，对推导中使用的符号做一个详细的说明，使推导的过程清晰易懂。我们用
<span class="math inline">\(L\)</span> 代表网络的层数，用 <span class="math inline">\(S_l\)</span> 代表第 <span class="math inline">\(l\)</span> 层的节点的个数；用 <span class="math inline">\(z^{(1)}\)</span> 和 <span class="math inline">\(a^{(1)}\)</span> 表示第 <span class="math inline">\(l\)</span> 层经过激活函数前/后的节点向量 (<span class="math inline">\(z^{(l)}_i\)</span> 和 <span class="math inline">\(a^{(l)}_i\)</span> 代表经过激活函数前/后节点 <span class="math inline">\(i\)</span> 的值)，根据以上的表示，<span class="math inline">\(a^{(1)}\)</span> 代表网络的输入 <span class="math inline">\(x\)</span>, <span class="math inline">\(a^{(L)}\)</span>代表网络的输出 <span class="math inline">\(y\)</span>, 也就是上图中的 <span class="math inline">\(h_{W,b}(x)\)</span>；用 <span class="math inline">\(W^{(l)}\)</span> 表示第<span class="math inline">\(l\)</span> 层与第<span class="math inline">\(l+1\)</span> 层之间的权值形成的矩阵，<span class="math inline">\(W^{(l)}_{ij}\)</span> 代表 <span class="math inline">\(l\)</span> 层的节点 <span class="math inline">\(j\)</span> 与<span class="math inline">\(l+1\)</span> 层的节点 <span class="math inline">\(i\)</span> 之间的权重(注意这种表示方式),用 <span class="math inline">\(b^{(l)}\)</span> 代表 <span class="math inline">\(l\)</span> 层到<span class="math inline">\(l+1\)</span>层之间的偏置向量。</p><p>有了上面的符号化表示，神经网络各层之间的关系，可以用简洁的向量矩阵形式来表达如下：</p><p><span class="math display">\[\begin{align}
z^{(l+1)} &= W^{(l)}a^{(l)}+b^{(l)} \label{eq3} \\
a^{(l+1)} &= f\left(z^{(l+1)}\right) \label{eq4}
\end{align}\]</span></p><p>根据以上的式子，我们就可以计算网络中每一层各个节点的值了，上述的过程称为「前向传播」(forward
propagation) 过程，在深度学习库中通常都被叫做「forward」。</p><p>通常，网络刚创建好时，我们随机初始化每两层之间的权值矩阵以及偏置向量，但是这样得到的网络，输出与实际的值差距太大。使用神经网络的目的，当然是想要网络的输出与实际的值差距尽可能小，随机初始化网络，显然不能满足这个目的。但是如何调整各层之间的权值矩阵以及偏置呢，这并不是一个很简单的问题，下面要推导的反向传播(backward
propagation) 算法就是解决这个问题的利器。</p><h1 id=正式开始推导>正式开始推导</h1><p>通常来说，如果想要得到一个较好的网络，需要有一批已知的训练数据，假设我们现在总共有<span class="math inline">\(m\)</span>个样本,即</p><p><span class="math display">\[\left\{(x^{(i)}, y^{(i)})\right\}, i=
1,2,\ldots,m\]</span></p><p>对于每一个样本来说，我们优化的目标为 ：</p><p><span class="math display">\[\begin{equation}\label{eq5}
J\left(W,b;x^{(i)}, y^{(i)}\right) = \min_{W,b} \frac{1}{2}\left\lVert
h\left(x^{(i)}\right)- y^{(i)} \right\rVert^2
\end{equation}\]</span></p><p>公式 <span class="math inline">\(\eqref{eq5}\)</span>
中，为了简化，我们用 <span class="math inline">\(h(x^{(i)})\)</span>
表示 $h_{W,b}(x^{(i)}) $</p><p>对于所有样本来说,我们需要最小化的目标函数为：</p><p><span class="math display">\[\begin{equation}
\begin{aligned}
J(W, b) &= \frac{1}{m}\sum_{i=1}^{m}J\left(W,b;x^{(i)},
y^{(i)}\right) + \frac{\lambda}{2}\sum_{l=1}^{L-1}\left\lVert W^{(l)}
\right\rVert_{F}^{2}\\
&= \frac{1}{m}\sum_{i=1}^{m}\frac{1}{2}\left\lVert h(x^{(i)}) -
y^{(i)}\right\rVert^2 + \frac{\lambda}{2}\sum_{l=1}^{L-1}\left\lVert
W^{(l)} \right\rVert_{F}^{2}
\end{aligned}\label{eq6}
\end{equation}\]</span></p><p>上述优化目标函数 <span class="math inline">\(\eqref{eq6}\)</span>，并不简单是各个样本优化目标的和，而是由两项构成：第一项为误差项，第二项称为「正则项」(英文叫「regularization
term」也称为weight decay
term)，用来控制各层权值矩阵的元素大小，防止权值矩阵过大，网络出现过拟合，这和曲线拟合中对参数使用正则道理是一样的，只不过曲线拟合中，参数是向量，这里的参数是矩阵。我们使用
<span class="math inline">\(F\)</span>
范数的平方来约束权重矩阵元素的大小，正则项前面的系数 <span class="math inline">\(\lambda\)</span> (称为weight decay parameter)
用来控制正则项与误差项之间的权重。另外，一般来说，只对权值矩阵进行正则，不对偏置进行正则。</p><h2 id=关于f范数的一点小知识>关于F范数的一点小知识</h2><p>为了计算目标函数对权重矩阵的偏导，有必要对 <span class="math inline">\(F\)</span> 范数 (也称为 <span class="math inline">\(F\)</span>-norm) 有所了解。假设矩阵 <span class="math inline">\(A\)</span> 是实数矩阵，大小为 <span class="math inline">\(m\times n\)</span>,其 <span class="math inline">\(F\)</span> 范数用公式可以表示为：</p><p><span class="math display">\[\begin{equation}
\lVert A \rVert_F = \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n} (a_{ij})^2}
\end{equation}\]</span></p><p>另外关于矩阵 <span class="math inline">\(A\)</span> 的 <span class="math inline">\(F\)</span> 范数对矩阵 <span class="math inline">\(A\)</span> 如何求导，有如下公式：</p><p><span class="math display">\[\begin{equation}\label{eq8}
\frac{\partial \lVert A \rVert_F^2 }{\partial A}= 2A
\end{equation}\]</span></p><h1 id=抽丝剥茧不断深入>抽丝剥茧，不断深入</h1><p>我们优化网络的目标是计算各层的权值矩阵以及偏置向量，使得优化目标函数取得最小值。根据<a href=https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95>梯度下降算法</a>，可以计算目标函数对各个参数的偏导，采用迭代方式来更新参数，最终得到最优的参数值。但是事实上，上述函数是非凸函数，梯度下降算法并不一定能够得到全局最优解(global
optimum)，一般只能得到局部最优解(local
optimum)。实际中得到的结果一般都是比较接近最优结果，在可以接受的范围之内，所以才使用梯度下降算法来优化神经网络。另外实际优化过程中，还有一些技巧，譬如加入
momentum 项，使得目标函数能够跳出local optimum 点，从而得到global
optimum。在实际的深度学习训练中，一般都会使用momentum
来加快收敛的速度，这里仅讨论最基本情况，对增加 momentum
项的情况不予讨论。</p><p>如果已经求得目标函数对各个函数的偏导数，那么各个参数的更新公式如下：</p><p><span class="math display">\[\begin{align}
W_{ij}^{(l)} &= W_{ij}^{(l)} - \alpha \frac{\partial
J(W,b)}{\partial W_{ij}^{(l)}} \label{eq9} \\
b_i^{(l)} &= b_i^{(l)} - \alpha \frac{\partial J(W, b)}{\partial
b_i^{(l)}} \label{10}
\end{align}\]</span></p><p>以上的式子中，<span class="math inline">\(\alpha\)</span>
称为「学习率」(learning rate)，用来控制权重和偏置项的更新幅度，如果
<span class="math inline">\(\alpha\)</span>
太大，网络的参数收敛速度快，但是可能出现来回震荡的情况，甚至不能收敛；如果
<span class="math inline">\(\alpha\)</span>
太小，网络收敛速度太慢，训练时间长。需要说明，权重矩阵以及偏置向量的学习率可以不一样，根据需要分别设置，实际上，Caffe
就是这么做的，可以在 prototxt
里面指定每层的权重以及偏置的学习率，其他的深度学习框架也可以类似设置。</p><p>从上面的公式可以看出，现在的关键是，如何计算目标函数对权重矩阵以及偏置项各个元素的偏导，根据目标函数
<span class="math inline">\(J(W, b)\)</span> 的计算公式 <span class="math inline">\(\eqref{eq6}\)</span>，可以得到目标函数<span class="math inline">\(J(W, b)\)</span>
对权重矩阵以及偏置项各元素的导数：</p><p><span class="math display">\[\begin{align}
\frac{\partial J(W, b) }{\partial W_{ij}^{(l)}} &=
\left[\frac{1}{m}\sum_{i=1}^{m}\frac{\partial J\left(W, b; x^{(i)},
y^{(i)}\right)}{\partial W_{ij}^{(l)}} \right] + \lambda W_{ij}^{(l)}
\label{eq11}\\
\frac{\partial J(W, b) }{\partial b_i^{(l)}} &=
\frac{1}{m}\sum_{i=1}^{m}\frac{\partial J\left(W, b; x^{(i)},
y^{(i)}\right)}{\partial b_i^{(l)}} \label{eq12}
\end{align}\]</span></p><p>上面两个公式中,公式 <span class="math inline">\(\eqref{eq11}\)</span>
后半部分可以参考前面对于矩阵范数求导的公式<span class="math inline">\(\eqref{eq8}\)</span> 得到。</p><p>观察以上公式，接下来的问题就是，如何求取样本目标函数对于权重矩阵以及偏置向量的偏导，也就是如何求
<span class="math inline">\(\frac{\partial}{\partial
W_{ij}^{(l)}}J\left(W,b;x^{(i)},y^{(i)}\right)\)</span> 以及 <span class="math inline">\(\frac{\partial}{\partial
b_{i}^{(l)}}J\left(W,b;x^{(i)},y^{(i)}\right)\)</span>？如果得到每个样本的目标函数对于各个参数的偏导，整个问题就解决了。</p><p>这就要用到我们前面所说的 back-propagation
的思想了，当我们把一个样本输入到网络，通过前向传播，得到最终的输出，最终输出与实际的值之间有误差，然后我们通过某种有组织有规律的方式把误差一层一层向前传播，得到误差相对于每一层参数的偏导，这是求解该问题的核心思想。</p><p>为了便于推导，再引入变量 <span class="math inline">\(\delta_{i}^{(l)}=\frac{\partial}{\partial
z_{i}^{(l)}}J\left(W,b;x^{(i)},y^{(i)}\right)\)</span>,
即最终的误差对每一层节点经过激活函数前的变量的偏导，用它来衡量某一层某个节点对最终误差的贡献量。</p><h1 id=计算辅助变量的值>计算辅助变量的值</h1><p>对于最后一层(第 <span class="math inline">\(L\)</span>
层)，我们可以很方便的计算 <span class="math inline">\(\delta_i^{(L)}\)</span>，详细推导如下：</p><p><span class="math display">\[\begin{equation}
\begin{aligned}
\delta_{i}^{(L)}&=\frac{\partial}{\partial
z_{i}^{(L)}}\left(\frac{1}{2}\left\lVert y - h(x) \right\rVert^2\right)
\\
&= \frac{\partial}{\partial a_{i}^{(L)}}\left(\frac{1}{2}\left\lVert
y - h(x) \right\rVert^2\right)\cdot \frac{\partial a_{i}^{(L)}}{\partial
z_{i}^{(L)}}\\
&=\frac{1}{2}\left[ \frac{\partial}{\partial
a_{i}^{(L)}}\sum_{j=1}^{S_L} \left(y_j - a_j^{(L)}\right)^2 \right]
\cdot \frac{\partial a_{i}^{(L)}}{\partial z_{i}^{(L)}}\\
&=-\left(y_{i}-a_{i}^{(L)}\right)f'\left(z_{i}^{(L)}\right)
\end{aligned}\label{eq13}
\end{equation}\]</span></p><p>上述公式 <span class="math inline">\(\eqref{eq13}\)</span> 中，<span class="math inline">\(f'\left(z_{i}^{(L)}\right)=a^{(L)}_{i}\left(1-a^{(L)}_{i}\right)\)</span>
，对于其他层也是如此计算，不再赘述。对于其他层(<span class="math inline">\(l=L-1,L-2,\cdots,2\)</span>)的辅助变量，计算就不那么容易了，因为输出误差并不直接和这些层的节点相关，所以我们需要构造关系，利用微积分里面的<a href=https://en.wikipedia.org/wiki/Chain_rule>链式法则</a> (chain
rule),具体计算过程如下：</p><p><span class="math display">\[\begin{equation}
\begin{aligned}
\delta_{i}^{(l)}&=\frac{\partial}{\partial z_{i}^{(l)}}J(W,b;x,y) \\
&=\sum_{j=1}^{s_{l+1}}\frac{\partial J}{\partial z_{j}^{(l+1)}}\cdot
\frac{\partial z_{j}^{(l+1)}}{\partial z_{i}^{(l)}}\\
&=\sum_{j=1}^{s_{l+1}}\delta_{j}^{(l+1)}\cdot
W_{ji}^{(l)}f'\left(z_{i}^{(l)}\right)\\
&=\left(\sum_{j=1}^{s_{l+1}}
W_{ji}^{(l)}\delta_{j}^{(l+1)}\right)\cdot
f'\left(z_{i}^{(l)}\right)
\end{aligned}\label{eq14}
\end{equation}\]</span></p><p>为了便于书写，上面公式<span class="math inline">\(\eqref{eq14}\)</span>中，<span class="math inline">\(J(W,b;x,y)=J\)</span>。求误差对 <span class="math inline">\(l\)</span>
层某个节点的偏导，无法直接求解，因为误差只和最后一层的节点有直接关系，但是如果我们已经知道了误差相对于下一层
(也就是 <span class="math inline">\(l+1\)</span> 层)
节点的偏导，而下一层节点和本层 (<span class="math inline">\(l\)</span>
层)直接相关，那么整个链条就可以打通了。关于如何由第一行得到第二行，我起初并没有正确得到，后来结合网上给的参考结果，逐渐想通如何计算。微积分中有针对多个中间变量的链式法则，其思想为：如果目标变量
(dependent varialbe)是三个中间变量(intermediate variable) <span class="math inline">\(P,Q,R\)</span> 的函数，而这三个中间变量又是自变量
(independent variable) <span class="math inline">\(x\)</span>
的函数，那么很容易证明下面的式子成立，</p><p><span class="math display">\[\begin{equation}\label{eq15}
\frac{\partial \, obj}{\partial x}=\frac{\partial \, obj}{\partial
P}\cdot \frac{\partial P}{\partial x}+\frac{\partial \, obj}{\partial
Q}\cdot \frac{\partial Q}{\partial x}+\frac{\partial \, obj}{\partial
R}\cdot \frac{\partial R}{\partial x}
\end{equation}\]</span></p><p>上述公式 <span class="math inline">\(\eqref{eq14}\)</span>
中第二行的求和符号就是这么来的，起初推导时少了求和符号，只求了误差相对于下一层节点
<span class="math inline">\(i\)</span>
的偏导，没有意识到下一层的每个节点其实与上一层的每个节点都有关系,
利用链式法则，我们就可以很容易求得误差相对于本层的偏导，这就是误差反传的思想。</p><p>根据我们前面的定义，上面的公式 <span class="math inline">\(\eqref{eq14}\)</span>，第二行求和符号里面的第一项<span class="math inline">\(\frac{\partial J}{\partial
z_{j}^{(l+1)}}\)</span>，等于 <span class="math inline">\(\delta_{j}^{(l+1)}\)</span>。第二项如何显式表达出来呢？结合公式
<span class="math inline">\(\eqref{eq3}\)</span> 和 <span class="math inline">\(\eqref{eq4}\)</span>，具体如下：</p><p><span class="math display">\[\begin{equation}
\begin{aligned}
z_{j}^{(l+1)}&= \left [\sum_{i=1}^{s_l} W_{ji}^{(l)}\cdot
a_{i}^{(l)} \right]+b_{j}^{(l)} \\
&=\left [\sum_{i=1}^{s_l} W_{ji}^{(l)}\cdot
f\left(z_i^{(l)}\right)\right]+b_{j}^{(l)}
\end{aligned}\label{eq16}
\end{equation}\]</span></p><p>有了公式 <span class="math inline">\(\eqref{eq16}\)</span>，那么公式
<span class="math inline">\(\eqref{eq14}\)</span>
第二行第二项偏导就很容易得到了：</p><p><span class="math display">\[\begin{equation}
\frac{\partial z_{j}^{(l+1)}}{\partial
z_{i}^{(l)}}=W_{ji}^{(l)}f'(z_{i}^{(l)})
\end{equation}\]</span></p><p>这就是公式 <span class="math inline">\(\eqref{eq14}\)</span>
第三行第二项的来历。</p><h1 id=计算误差相对于矩阵元素和偏置向量元素的偏导>计算误差相对于矩阵元素和偏置向量元素的偏导</h1><p>有了以上的铺垫，我们现在可以计算误差相对于矩阵元素以及偏置向量元素的偏导了。</p><p><span class="math display">\[\begin{equation}
\begin{aligned}
\frac{\partial J}{\partial W_{ij}^{(l)}}&=\frac{\partial J}{\partial
z_{i}^{(l+1)}}\cdot\frac{\partial z_{i}^{(l+1)}}{\partial W_{ij}^{(l)}}
\\
&= \delta_{i}^{(l+1)}\cdot a_{j}^{(l)}
\end{aligned}
\end{equation}\]</span></p><p><span class="math display">\[\begin{equation}
\begin{aligned}
\frac{\partial J}{\partial b_{i}^{(l)}}&=\frac{\partial J}{\partial
z_{i}^{(l+1)}}\cdot\frac{\partial z_{i}^{(l+1)}}{\partial b_{i}^{(l)}}\\
&= \delta_{i}^{(l+1)}\cdot 1 \\
&= \delta_{i}^{(l+1)}
\end{aligned}
\end{equation}\]</span></p><h1 id=向量化表示>向量化表示</h1><p>对于输出层：</p><p><span class="math display">\[\begin{equation}
\delta^{(L)}=-\left(y-a^{(L)}\right)\cdot f'\left(z^{(L)}\right)
\end{equation}\]</span></p><p>对于其他层 (<span class="math inline">\(l=L-1,L-2,\cdots,2\)</span>):</p><p><span class="math display">\[\begin{equation}
\delta^{(l)}=\left[\left(W^{(l)}\right)^{T}\delta^{(l+1)}\right]\cdot
f'\left(z^{(l)}\right)
\end{equation}\]</span></p><p>权重以及偏置更新公式：</p><p><span class="math display">\[\begin{align}
\frac{\partial J}{\partial W^{(l)}} &=
\delta^{(l+1)}\left(a^{(l)}\right)^{T}\\
\frac{\partial J}{\partial b^{(l)}} &= \delta^{(l+1)}
\end{align}\]</span></p><h1 id=把所有公式整合在一起>把所有公式整合在一起</h1><p>现在我们可以把所有的公式结合在一起，得出最终的参数更新公式了。</p><ol type=1><li><p>初始化，对于所有层 (<span class="math inline">\(l=1,2,\cdots,L-1\)</span>)，令 <span class="math inline">\(\Delta W^{(l)}=0\)</span>, <span class="math inline">\(\Delta
b^{(l)}=0\)</span>,前一项是一个矩阵，后一项是一个向量，分别代表对权重矩阵以及偏置向量的更新量。</p></li><li><p>对于一个batch的所有训练样本 (for i=1 to m)</p><ul><li>使用误差反传计算 <span class="math inline">\(\nabla_{W^{(l)}}J\left(W,b;x^{(i)},y^{(i)}\right)\)</span>
和 <span class="math inline">\(\nabla_{b^{(l)}}J\left(W,b;x^{(i)},y^{(i)}\right)\)</span></li><li><span class="math inline">\(\Delta W^{(l)}:= \Delta
W^{(l)}+\nabla_{W^{(l)}}J\left(W,b;x^{(i)},y^{(i)}\right)\)</span></li><li><span class="math inline">\(\Delta b^{(l)}:= \Delta
b^{(l)}+\nabla_{b^{(l)}}J\left(W,b;x^{(i)},y^{(i)}\right)\)</span></li></ul></li><li><p>更新参数</p></li></ol><p><span class="math display">\[\begin{align}
W^{(l)} &= W^{(l)}-\alpha\left[\left(\frac{1}{m}\Delta
W^{(l)}\right) + \lambda W^{(l)} \right] \\
b^{(l)} &= b^{(l)}-\alpha\left[\frac{1}{m}\Delta b^{(l)}\right]
\end{align}\]</span></p><p>至此，误差反传以及参数更新的全部内容完成！</p><h1 id=参考资料>参考资料</h1><ul><li><a href=http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm class=uri>http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm</a></li><li><a href=https://en.wikipedia.org/wiki/Backpropagation class=uri>https://en.wikipedia.org/wiki/Backpropagation</a></li><li><a href=http://neuralnetworksanddeeplearning.com/chap2.html class=uri>http://neuralnetworksanddeeplearning.com/chap2.html</a></li></ul></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/&amp;title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ad%e8%af%af%e5%b7%ae%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%28back%20propagation%29%e7%ae%97%e6%b3%95%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/&amp;text=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ad%e8%af%af%e5%b7%ae%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%28back%20propagation%29%e7%ae%97%e6%b3%95%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://reddit.com/submit/?url=https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/&amp;resubmit=true&amp;title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ad%e8%af%af%e5%b7%ae%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%28back%20propagation%29%e7%ae%97%e6%b3%95%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86" title="Submit to Reddit" aria-label="Submit to Reddit"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M201.5 305.5c-13.8.0-24.9-11.1-24.9-24.6.0-13.8 11.1-24.9 24.9-24.9 13.6.0 24.6 11.1 24.6 24.9.0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4.0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7.0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9.0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5.0 52.6 59.2 95.2 132 95.2 73.1.0 132.3-42.6 132.3-95.2.0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6.0-2.2-2.2-6.1-2.2-8.3.0-2.5 2.5-2.5 6.4.0 8.6 22.8 22.8 87.3 22.8 110.2.0 2.5-2.2 2.5-6.1.0-8.6-2.2-2.2-6.1-2.2-8.3.0zm7.7-75c-13.6.0-24.6 11.1-24.6 24.9.0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.1 24.9-24.6.0-13.8-11-24.9-24.9-24.9z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://pinterest.com/pin/create/bookmarklet/?url=https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/&amp;description=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ad%e8%af%af%e5%b7%ae%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%28back%20propagation%29%e7%ae%97%e6%b3%95%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86" title="Pin on Pinterest" aria-label="Pin on Pinterest"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M496 256c0 137-111 248-248 248-25.6.0-50.2-3.9-73.4-11.1 10.1-16.5 25.2-43.5 30.8-65 3-11.6 15.4-59 15.4-59 8.1 15.4 31.7 28.5 56.8 28.5 74.8.0 128.7-68.8 128.7-154.3.0-81.9-66.9-143.2-152.9-143.2-107 0-163.9 71.8-163.9 150.1.0 36.4 19.4 81.7 50.3 96.1 4.7 2.2 7.2 1.2 8.3-3.3.8-3.4 5-20.3 6.9-28.1.6-2.5.3-4.7-1.7-7.1-10.1-12.5-18.3-35.3-18.3-56.6.0-54.7 41.4-107.6 112-107.6 60.9.0 103.6 41.5 103.6 100.9.0 67.1-33.9 113.6-78 113.6-24.3.0-42.6-20.1-36.7-44.8 7-29.5 20.5-61.3 20.5-82.6.0-19-10.2-34.9-31.4-34.9-24.9.0-44.9 25.7-44.9 60.2.0 22 7.4 36.8 7.4 36.8s-24.5 103.8-29 123.2c-5 21.4-3 51.6-.9 71.2C65.4 450.9.0 361.1.0 256 0 119 111 8 248 8s248 111 248 248z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.facebook.com/sharer/sharer.php?u=https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/&amp;quote=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ad%e8%af%af%e5%b7%ae%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%28back%20propagation%29%e7%ae%97%e6%b3%95%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86" title="Share on Facebook" aria-label="Share on Facebook"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14.0 55.52 4.84 55.52 4.84v61h-31.28c-30.8.0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/&amp;subject=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ad%e8%af%af%e5%b7%ae%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%28back%20propagation%29%e7%ae%97%e6%b3%95%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://api.whatsapp.com/send?text=https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/&amp;resubmit=true&amp;title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ad%e8%af%af%e5%b7%ae%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%28back%20propagation%29%e7%ae%97%e6%b3%95%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86" title aria-label><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4.0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3.0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2.0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2.0-101.7 82.8-184.5 184.6-184.5 49.3.0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5.0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8s-14.3 18-17.6 21.8c-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2s-9.7 1.4-14.8 6.9c-5.1 5.6-19.4 19-19.4 46.3.0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://t.me/share/url?url=https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/&amp;resubmit=true&amp;title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ad%e8%af%af%e5%b7%ae%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%28back%20propagation%29%e7%ae%97%e6%b3%95%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86" title aria-label><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M248 8C111.033 8 0 119.033.0 256S111.033 504 248 504 496 392.967 496 256 384.967 8 248 8zM362.952 176.66c-3.732 39.215-19.881 134.378-28.1 178.3-3.476 18.584-10.322 24.816-16.948 25.425-14.4 1.326-25.338-9.517-39.287-18.661-21.827-14.308-34.158-23.215-55.346-37.177-24.485-16.135-8.612-25 5.342-39.5 3.652-3.793 67.107-61.51 68.335-66.746.153-.655.3-3.1-1.154-4.384s-3.59-.849-5.135-.5q-3.283.746-104.608 69.142-14.845 10.194-26.894 9.934c-8.855-.191-25.888-5.006-38.551-9.123-15.531-5.048-27.875-7.717-26.8-16.291q.84-6.7 18.45-13.7 108.446-47.248 144.628-62.3c68.872-28.647 83.183-33.623 92.511-33.789 2.052-.034 6.639.474 9.61 2.885a10.452 10.452.0 013.53 6.716A43.765 43.765.0 01362.952 176.66z"/></svg></span></a></section><h2 class="mt-8 text-2xl font-extrabold mb-10">Related</h2><section class="w-full grid gap-4 sm:grid-cols-2 md:grid-cols-3"><a href=/2022/04/28/sqrt_without_using_builtin_sqrt/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/2022/04/28/sqrt_without_using_builtin_sqrt/>How to Calculate Square Root without Using sqrt()?</div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime="2022-04-28 21:45:01 +0800 +0800">28 April 2022</time><span class="px-2 text-primary-500">&#183;</span><time datetime="2022-06-15 17:22:39.848521708 +0200 CEST">Updated: 15 June 2022</time><span class="px-2 text-primary-500">&#183;</span><span>748 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">4 mins</span></div></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/2017/10/24/image-similarity-measure-image-retrieval/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/2017/10/24/image-similarity-measure-image-retrieval/>Similarity Measurement in Image Retrieval</div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime="2017-10-24 20:38:52 +0800 +0800">24 October 2017</time><span class="px-2 text-primary-500">&#183;</span><time datetime="2022-02-27 06:04:16.10108165 +0100 CET">Updated: 27 February 2022</time><span class="px-2 text-primary-500">&#183;</span><span>482 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">3 mins</span></div></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/2016/11/25/thought-on-optimization-for-cnn/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/2016/11/25/thought-on-optimization-for-cnn/>关于神经网络优化的一些思考</div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime="2016-11-25 00:00:00 +0000 UTC">25 November 2016</time><span class="px-2 text-primary-500">&#183;</span><time datetime="2021-12-05 17:41:13.306372289 +0100 CET">Updated: 5 December 2021</time><span class="px-2 text-primary-500">&#183;</span><span>30 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div></div></div><div class="px-6 pt-4 pb-2"></div></div></a></section></div><script>var oid="views_posts/back-propagation-in-mlp-explained.md",oid_likes="likes_posts/back-propagation-in-mlp-explained.md"</script><script type=text/javascript src=/js/page.min.b06a29d42a4ed16787978e2eee1e8c797b7698db2bc14ccee78f5c80ac566fc996190a73ad80a5e987558474b20b96fa38f7d85b405f165ff72b7b163c5ad11b.js integrity="sha512-sGop1CpO0WeHl44u7h6MeXt2mNsrwUzO549cgKxWb8mWGQpzrYCl6YdVhHSyC5b6OPfYW0BfFl/3K3sWPFrRGw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/2016/03/17/common_methon_in_python_os_module/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Commonly-used Methods in Python OS Package</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2016-03-17 00:00:00 +0000 UTC">17 March 2016</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/2015/10/21/python-pip-install/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Windows 系统下 Python 以及 Pip 的安装和使用</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2015-10-21 14:29:00 +0800 +0800">21 October 2015</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">© 2017 - 2024 ❤️ jdhao</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.62060bb247f4de2b6dde45903668fefb68d792f365587605177b1227c0cf43588701edaca0cb40e2c8e2789bd5ce67c1d2a215b9fb258c3496a7cd25e7cb5fdf.js integrity="sha512-YgYLskf03itt3kWQNmj++2jXkvNlWHYFF3sSJ8DPQ1iHAe2soMtA4sjieJvVzmfB0qIVufsljDSWp80l58tf3w=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://jdhao.github.io/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>